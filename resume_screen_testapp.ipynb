{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fca3c723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting resume_screen_testapp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile resume_screen_testapp.py\n",
    "\n",
    "import streamlit as st\n",
    "import base64\n",
    "# ASTRO - automated system for talent recruitment optimisation\n",
    "#automated screening tool for recruitment operations\n",
    "#AURA - Automated Utility for Resume Assessment\n",
    "\n",
    "def add_bg_from_url():\n",
    "    st.markdown(\n",
    "         f\"\"\"\n",
    "         <style>\n",
    "         .stApp {{\n",
    "             #background-image: url(\"https://cdn.pixabay.com/photo/2016/10/05/03/36/blue-1716030_1280.jpg\");\n",
    "             background-image: linear-gradient(rgba(255, 255, 255, 0.5), rgba(255, 255, 255, 0.75)), url(\"https://cdn.pixabay.com/photo/2016/10/29/10/12/purple-1780371_1280.png\");\n",
    "             background-attachment: fixed;\n",
    "             background-size: cover\n",
    "         }}\n",
    "         </style>\n",
    "         \"\"\",\n",
    "         unsafe_allow_html=True\n",
    "     )\n",
    "\n",
    "add_bg_from_url() \n",
    "\n",
    "\n",
    "# Upload resumes\n",
    "uploaded_files = st.file_uploader(\"Upload Resumes\", accept_multiple_files=True, type=[\"pdf\", \"docx\", \"txt\"])\n",
    "\n",
    "if uploaded_files:\n",
    "    st.write(\"Files uploaded successfully.\")\n",
    "    for uploaded_file in uploaded_files:\n",
    "        try:\n",
    "            st.write(\"Filename: \", uploaded_file.name)\n",
    "            st.write(\"File type: \", uploaded_file.type)\n",
    "            st.write(\"File size: \", uploaded_file.size)\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error processing file {uploaded_file.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f144d3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting resume_screen_testapp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile resume_screen_testapp.py\n",
    "import streamlit as st\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Credentials from the .env file\n",
    "SENDER_EMAIL = os.getenv('SENDER_EMAIL')\n",
    "SENDER_PASSWORD = os.getenv('SENDER_PASSWORD')\n",
    "\n",
    "# Ensure credentials are loaded properly\n",
    "if not SENDER_EMAIL or not SENDER_PASSWORD:\n",
    "    st.error(\"Failed to load environment variables. Please check the .env file.\")\n",
    "else:\n",
    "    # Function to send email\n",
    "    def send_email(to_email, message):\n",
    "        try:\n",
    "            # Create the email message\n",
    "            msg = MIMEText(message)\n",
    "            msg['Subject'] = 'Message from Resume Screening App'\n",
    "            msg['From'] = SENDER_EMAIL\n",
    "            msg['To'] = to_email\n",
    "\n",
    "            # Connect to Gmail's SMTP server\n",
    "            server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "            server.starttls()\n",
    "            server.login(SENDER_EMAIL, SENDER_PASSWORD)\n",
    "            server.sendmail(SENDER_EMAIL, to_email, msg.as_string())\n",
    "            server.quit()\n",
    "\n",
    "            st.success(f\"Email sent successfully to {to_email}!\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"Failed to send email: {str(e)}\")\n",
    "\n",
    "    # Streamlit UI\n",
    "    st.title(\"Send Email from Resume Screening App\")\n",
    "\n",
    "    # User inputs\n",
    "    recipient_email = st.text_input(\"Enter recipient email:\")\n",
    "    message = st.text_area(\"Enter your message:\")\n",
    "\n",
    "    # Send email when the button is clicked\n",
    "    if st.button(\"Send Email\"):\n",
    "        if recipient_email and message:\n",
    "            send_email(recipient_email, message)\n",
    "        else:\n",
    "            st.error(\"Please enter both recipient email and a message.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8ec395fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting resume_screen_testapp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile resume_screen_testapp.py\n",
    "import streamlit as st\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Credentials from the .env file\n",
    "SENDER_EMAIL = os.getenv('SENDER_EMAIL')\n",
    "SENDER_PASSWORD = os.getenv('SENDER_PASSWORD')\n",
    "\n",
    "# Ensure credentials are loaded properly\n",
    "if not SENDER_EMAIL or not SENDER_PASSWORD:\n",
    "    st.error(\"Failed to load environment variables. Please check the .env file.\")\n",
    "else:\n",
    "    # Function to send email\n",
    "    def send_email(sender_email, sender_password, recipient_email, subject, message, content_type='plain'):\n",
    "        try:\n",
    "            # Create the MIMEMultipart object and set headers\n",
    "            msg = MIMEMultipart()\n",
    "            msg['From'] = sender_email\n",
    "            msg['To'] = recipient_email\n",
    "            msg['Subject'] = subject\n",
    "\n",
    "            # Attach message body (plain or HTML depending on content_type)\n",
    "            mime_text = MIMEText(message, content_type)\n",
    "            msg.attach(mime_text)\n",
    "\n",
    "            # Connect to Gmail's SMTP server using STARTTLS\n",
    "            server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "            server.starttls()\n",
    "            server.login(sender_email, sender_password)\n",
    "\n",
    "            # Send the email\n",
    "            server.sendmail(sender_email, recipient_email, msg.as_string())\n",
    "            server.quit()\n",
    "\n",
    "            # Return success\n",
    "            return True, None\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error sending email: {e}\"\n",
    "            print(error_message)\n",
    "            return False, error_message\n",
    "\n",
    "    # Streamlit UI\n",
    "    st.title(\"Send Email from Resume Screening App\")\n",
    "\n",
    "    # User inputs\n",
    "    recipient_email = st.text_input(\"Enter recipient email:\")\n",
    "    subject = st.text_input(\"Enter email subject:\")\n",
    "    message = st.text_area(\"Enter your message:\")\n",
    "    \n",
    "    # Choose content type (plain text or HTML)\n",
    "    content_type = st.selectbox(\"Select content type:\", ['plain', 'html'])\n",
    "\n",
    "    # Send email when the button is clicked\n",
    "    if st.button(\"Send Email\"):\n",
    "        if recipient_email and message and subject:\n",
    "            is_sent, error = send_email(SENDER_EMAIL, SENDER_PASSWORD, recipient_email, subject, message, content_type)\n",
    "            if is_sent:\n",
    "                st.success(f\"Email sent successfully to {recipient_email}!\")\n",
    "            else:\n",
    "                st.error(f\"Failed to send email: {error}\")\n",
    "        else:\n",
    "            st.error(\"Please enter recipient email, subject, and message.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "103ed066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#automated screening tool for recruitment operations (ASTRO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "556d9027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#!streamlit run resume_screen_testapp.py\n",
    "\n",
    "!streamlit run resume_screen_testapp.py --server.enableXsrfProtection false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1f8e134f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting resume_screen_testapp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile resume_screen_testapp.py\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "st.set_page_config(layout=\"wide\")\n",
    "\n",
    "# Path to your image\n",
    "image_path = \"C:/Users/weichee/Documents/ai bootcamp/headerc.png\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Create a single column (use st.columns() correctly)\n",
    "col1 = st.columns(1)[0]\n",
    "\n",
    "# Display the image in the column\n",
    "with col1:\n",
    "    st.image(image, caption=\"\", use_column_width=True)\n",
    "\n",
    "\n",
    "# Load images using PIL\n",
    "image_path1 = \"C:/Users/weichee/Documents/ai bootcamp/data extraction button 1.png\"\n",
    "image_path2 = \"C:/Users/weichee/Documents/ai bootcamp/data extraction button 2.png\"\n",
    "image_path3 = \"C:/Users/weichee/Documents/ai bootcamp/blank purple2.png\"\n",
    "\n",
    "\n",
    "image1 = Image.open(image_path1)\n",
    "image2 = Image.open(image_path2)\n",
    "image3 = Image.open(image_path3)\n",
    "\n",
    "# Section title\n",
    "st.markdown(\"## Choose Your Resume Extraction Method\")\n",
    "\n",
    "# Create columns for the buttons\n",
    "col1, col2, col3, col4 = st.columns(4)\n",
    "\n",
    "# Display the images with a \"Get Started\" button below each\n",
    "with col1:\n",
    "    st.image(image3, caption=\"\", use_column_width=True)\n",
    "    \n",
    "with col2:\n",
    "    st.image(image1, caption=\"\", use_column_width=True)\n",
    "    if st.button(\"Get Started\", key=\"simulated\"):\n",
    "        st.session_state[\"button_clicked\"] = \"Simulated Website Extraction\"\n",
    "\n",
    "with col3:\n",
    "    st.image(image2, caption=\"\", use_column_width=True)\n",
    "    if st.button(\"Get Started\", key=\"manual\"):\n",
    "        st.session_state[\"button_clicked\"] = \"Manual Upload\"\n",
    "        \n",
    "with col4:\n",
    "    st.image(image3, caption=\"\", use_column_width=True)\n",
    "\n",
    "# Check which button was clicked and display the corresponding content\n",
    "if st.session_state.get(\"button_clicked\") == \"Simulated Website Extraction\":\n",
    "    st.header(\"Simulated Website Extraction\")\n",
    "    st.write(\"This option simulates resume and job description extraction as if from a website.\")\n",
    "    # Your mock extraction code here...\n",
    "\n",
    "elif st.session_state.get(\"button_clicked\") == \"Manual Upload\":\n",
    "    st.header(\"Manual Upload\")\n",
    "    uploaded_resumes = st.file_uploader(\"Upload Resumes\", accept_multiple_files=True)\n",
    "    job_description = st.text_area(\"Enter Job Description\")\n",
    "    # Your manual upload processing code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac22af46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29ad187d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting resume_screen_testapp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile resume_screen_testapp.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import docx2txt\n",
    "import PyPDF2\n",
    "import io\n",
    "\n",
    "# Function to extract text from PDF files\n",
    "def extract_text_from_pdf(file):\n",
    "    text = \"\"\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to extract text from DOCX files\n",
    "def extract_text_from_docx(file):\n",
    "    return docx2txt.process(file)\n",
    "\n",
    "# Function to extract text from TXT files\n",
    "def extract_text_from_txt(file):\n",
    "    return file.read().decode(\"utf-8\")\n",
    "\n",
    "# Function to extract text from different file types\n",
    "def extract_text(uploaded_file):\n",
    "    if uploaded_file.type == \"application/pdf\":\n",
    "        return extract_text_from_pdf(uploaded_file)\n",
    "    elif uploaded_file.type == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\":\n",
    "        return extract_text_from_docx(uploaded_file)\n",
    "    elif uploaded_file.type == \"text/plain\":\n",
    "        return extract_text_from_txt(uploaded_file)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Function to extract key information from job description and resumes\n",
    "def extract_skills(text):\n",
    "    skills = ['Databricks', 'Spark', 'Kafka', 'SQL', 'MongoDB', 'ElasticSearch', 'REST', 'RPC', 'Kubernetes', 'Flask', 'Postgres']\n",
    "    extracted_skills = [skill for skill in skills if skill.lower() in text.lower()]\n",
    "    return extracted_skills\n",
    "\n",
    "# Function to calculate similarity score\n",
    "def calculate_similarity(job_desc, resume_text):\n",
    "    vectorizer = TfidfVectorizer().fit([job_desc])\n",
    "    job_desc_vector = vectorizer.transform([job_desc])\n",
    "    resume_vector = vectorizer.transform([resume_text])\n",
    "    cosine_sim = cosine_similarity(job_desc_vector, resume_vector)\n",
    "    return cosine_sim[0][0]\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"Resume Matcher\")\n",
    "\n",
    "# Input for job description\n",
    "job_description = st.text_area(\"Enter Job Description\")\n",
    "\n",
    "# Upload resumes\n",
    "uploaded_files = st.file_uploader(\"Upload Resumes\", accept_multiple_files=True, type=[\"pdf\", \"docx\", \"txt\"])\n",
    "\n",
    "# Input for number of top resumes to display\n",
    "top_n = st.number_input(\"Enter the number of top resumes to display\", min_value=1, value=5, step=1)\n",
    "\n",
    "# Add a button to run the resume matching\n",
    "if st.button(\"Match Resumes\"):\n",
    "    if job_description and uploaded_files:\n",
    "        results = []\n",
    "        resume_texts = []\n",
    "        filenames = []\n",
    "\n",
    "        for uploaded_file in uploaded_files:\n",
    "            try:\n",
    "                file_content = extract_text(uploaded_file)\n",
    "                resume_texts.append(file_content)\n",
    "                filenames.append(uploaded_file.name)\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error processing file {uploaded_file.name}: {e}\")\n",
    "\n",
    "        if resume_texts:\n",
    "            scores = [calculate_similarity(job_description, resume) for resume in resume_texts]\n",
    "            results = [{\"Filename\": filenames[i], \"Score\": scores[i]} for i in range(len(scores))]\n",
    "            results = sorted(results, key=lambda x: x[\"Score\"], reverse=True)[:top_n]\n",
    "\n",
    "            # Convert results to DataFrame and reset index to start from 1\n",
    "            df_results = pd.DataFrame(results)\n",
    "            df_results.index = df_results.index + 1\n",
    "            st.write(f\"Top {top_n} Resumes\")\n",
    "            st.dataframe(df_results)\n",
    "\n",
    "            # Display detailed breakdown on click\n",
    "            selected_resume = st.selectbox(\"Select a Resume to View Details\", df_results[\"Filename\"])\n",
    "            if selected_resume:\n",
    "                selected_text = resume_texts[filenames.index(selected_resume)]\n",
    "                extracted_skills = extract_skills(selected_text)\n",
    "                st.write(\"**Filename:**\", selected_resume)\n",
    "                st.write(\"**Score:**\", df_results[df_results[\"Filename\"] == selected_resume].iloc[0][\"Score\"])\n",
    "                st.write(\"**Skills:**\", \", \".join(extracted_skills))\n",
    "    else:\n",
    "        st.error(\"Please enter a job description and upload at least one resume.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "573f1cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting resume_screen_testapp2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile resume_screen_testapp2.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import docx2txt\n",
    "from PyPDF2 import PdfReader\n",
    "import docx\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"Resume Matcher\")\n",
    "\n",
    "\n",
    "\n",
    "#m = st.markdown(\"\"\"\n",
    "#<style>\n",
    "#div.stButton > button:first-child {\n",
    "#    background-color: #0099ff;\n",
    "#    color:#ffffff;\n",
    "#}\n",
    "#div.stButton > button:hover {\n",
    "#    background-color: #00ff00;\n",
    "#    color:#ff0000;\n",
    "#    }\n",
    "#</style>\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "tab1, tab2= st.tabs([\"Simulated Website Extraction\", \"Upload Resumes & Job Description\"])\n",
    "\n",
    "with tab1:\n",
    "    st.header(\"Simulated Website Extraction\")\n",
    "    st.write(\"This tab simulates resume and job description extraction as if from a website.\")\n",
    "\n",
    "\n",
    "    # Input for job title (smaller text input)\n",
    "    job_title = st.text_input(\"Enter Job Title\", max_chars=50)\n",
    "\n",
    "    # Input for date range\n",
    "    start_date = st.date_input(\"Start Date (Submission)\")\n",
    "    end_date = st.date_input(\"End Date (Submission)\")\n",
    "\n",
    "    # Input for number of top resumes to display\n",
    "    top_n = st.number_input(\"Enter number of top resumes to display\", min_value=1, value=3)\n",
    "\n",
    "    # Function to filter resumes based on the date range\n",
    "    def filter_resumes_by_date(resume_folder, start_date, end_date):\n",
    "        filtered_resumes = []\n",
    "        for file_name in os.listdir(resume_folder):\n",
    "            file_path = os.path.join(resume_folder, file_name)\n",
    "            if not file_name.endswith(\".txt\"):  # Assuming all resumes are non-txt files\n",
    "                modification_time = os.path.getmtime(file_path)\n",
    "                modification_date = datetime.fromtimestamp(modification_time).date()\n",
    "                if start_date <= modification_date <= end_date:\n",
    "                    filtered_resumes.append(file_name)\n",
    "        return filtered_resumes\n",
    "\n",
    "    # Function to read different resume formats\n",
    "    def read_resume(file_path):\n",
    "        if file_path.endswith(\".pdf\"):\n",
    "            return read_pdf(file_path)\n",
    "        elif file_path.endswith(\".doc\") or file_path.endswith(\".docx\"):\n",
    "            return read_word(file_path)\n",
    "        elif file_path.endswith(\".txt\"):\n",
    "            return read_txt(file_path)\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    # Function to read PDF files\n",
    "    def read_pdf(file_path):\n",
    "        text = \"\"\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            reader = PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "        return text\n",
    "\n",
    "    # Function to read Word documents\n",
    "    def read_word(file_path):\n",
    "        doc = docx.Document(file_path)\n",
    "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "    # Function to read text files\n",
    "    def read_txt(file_path):\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                return file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, \"r\", encoding=\"ISO-8859-1\") as file:\n",
    "                return file.read()\n",
    "\n",
    "    # Function to compute cosine similarity\n",
    "    def compute_similarity(job_description, resumes):\n",
    "        documents = [job_description] + resumes\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "        cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "        return cosine_sim\n",
    "\n",
    "    # Button to trigger resume matching\n",
    "    #st.button(label=\"Match Resumes\", style=\"background-color: #DD3300; color:#eeffee; border-radius: 0.75rem;\")\n",
    "    st.button(\"My button\", type=\"primary\",use_container_width=True)\n",
    "    if st.button(\"Match Resumes\",type=\"secondary\"):\n",
    "        if job_title:\n",
    "            # Define the path based on the job title\n",
    "            folder_path = os.path.join(\"C:\\\\Users\\\\weichee\\\\Documents\\\\ai bootcamp\", job_title)\n",
    "\n",
    "            # Check if the folder exists\n",
    "            if os.path.exists(folder_path):\n",
    "                # Extract the job description\n",
    "                job_description_file = os.path.join(folder_path, f\"{job_title}.txt\")\n",
    "                if os.path.exists(job_description_file):\n",
    "                    with open(job_description_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                        job_description = file.read()\n",
    "\n",
    "                    # Filter resumes by date\n",
    "                    filtered_resumes = filter_resumes_by_date(folder_path, start_date, end_date)\n",
    "                    if filtered_resumes:\n",
    "                        # Read the content of each resume\n",
    "                        resumes_content = []\n",
    "                        for resume in filtered_resumes:\n",
    "                            resume_content = read_resume(os.path.join(folder_path, resume))\n",
    "                            resumes_content.append(resume_content)\n",
    "\n",
    "                        # Compute cosine similarity\n",
    "                        similarity_scores = compute_similarity(job_description, resumes_content)\n",
    "\n",
    "                        # Combine the resumes with their similarity scores\n",
    "                        resume_scores = list(zip(filtered_resumes, similarity_scores))\n",
    "\n",
    "                        # Sort resumes by similarity score in descending order\n",
    "                        sorted_resumes = sorted(resume_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                        # Display the top N resumes based on the user's input\n",
    "                        st.subheader(f\"Top {top_n} Resumes\")\n",
    "                        for i, (resume, score) in enumerate(sorted_resumes[:top_n]):\n",
    "                            st.write(f\"{i + 1}. {resume} - Similarity Score: {score:.4f}\")\n",
    "                    else:\n",
    "                        st.warning(\"No resumes found within the specified date range.\")\n",
    "                else:\n",
    "                    st.error(\"Job description file not found.\")\n",
    "            else:\n",
    "                st.error(\"Job title folder not found.\")\n",
    "        else:\n",
    "            st.error(\"Please enter a job title.\")\n",
    "            \n",
    "with tab2:\n",
    "    st.header(\"Upload Resumes & Job Description\")\n",
    "    uploaded_resumes = st.file_uploader(\"Upload Resumes\", accept_multiple_files=True)\n",
    "    job_description = st.text_area(\"Enter Job Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "03bc35cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run resume_screen_testapp2.py --server.enableXsrfProtection false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cc0971f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting resume_screen_testapp2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile resume_screen_testapp2.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "import PyPDF2\n",
    "import io\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv('.env')\n",
    "\n",
    "# Fetch the OpenAI API key from the environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Function to extract text from PDF files\n",
    "def extract_text_from_pdf(file):\n",
    "    text = \"\"\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to extract text from DOCX files\n",
    "def extract_text_from_docx(file):\n",
    "    return docx2txt.process(file)\n",
    "\n",
    "# Function to extract text from TXT files\n",
    "def extract_text_from_txt(file):\n",
    "    return file.read().decode(\"utf-8\")\n",
    "\n",
    "# Function to extract text from different file types\n",
    "def extract_text(uploaded_file):\n",
    "    if uploaded_file.type == \"application/pdf\":\n",
    "        return extract_text_from_pdf(uploaded_file)\n",
    "    elif uploaded_file.type == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\":\n",
    "        return extract_text_from_docx(uploaded_file)\n",
    "    elif uploaded_file.type == \"text/plain\":\n",
    "        return extract_text_from_txt(uploaded_file)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"Resume Matcher with Advanced Retrieval\")\n",
    "\n",
    "# Input for job description\n",
    "job_description = st.text_area(\"Enter Job Description\")\n",
    "\n",
    "# Upload resumes\n",
    "uploaded_files = st.file_uploader(\"Upload Resumes\", accept_multiple_files=True, type=[\"pdf\", \"docx\", \"txt\"])\n",
    "\n",
    "# Input for number of top resumes to display\n",
    "top_n = st.number_input(\"Enter the number of top resumes to display\", min_value=1, value=5, step=1)\n",
    "\n",
    "# Add a button to run the resume matching\n",
    "if st.button(\"Match Resumes\"):\n",
    "    if job_description and uploaded_files:\n",
    "        resume_texts = []\n",
    "        filenames = []\n",
    "\n",
    "        for uploaded_file in uploaded_files:\n",
    "            try:\n",
    "                file_content = extract_text(uploaded_file)\n",
    "                resume_texts.append(file_content)\n",
    "                filenames.append(uploaded_file.name)\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error processing file {uploaded_file.name}: {e}\")\n",
    "\n",
    "        # Split resumes into semantic chunks for better retrieval (pre-retrieval process)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        \n",
    "        # Wrap text content into Document objects for LangChain processing\n",
    "        documents = [Document(page_content=text, metadata={\"filename\": filenames[i]}) for i, text in enumerate(resume_texts)]\n",
    "        splitted_resumes = []\n",
    "        for doc in documents:\n",
    "            chunks = text_splitter.split_text(doc.page_content)\n",
    "            for chunk in chunks:\n",
    "                splitted_resumes.append(Document(page_content=chunk, metadata=doc.metadata))\n",
    "\n",
    "        # Store chunks and embeddings in Chroma for retrieval\n",
    "        embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')  # Pass the API key here\n",
    "        vectordb = Chroma.from_documents(splitted_resumes, embeddings_model)\n",
    "\n",
    "        # Create a LangChain RetrievalQA pipeline\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            retriever=vectordb.as_retriever(),\n",
    "            llm=llm\n",
    "        )\n",
    "\n",
    "        # Generate unique summaries for each resume\n",
    "        results = []\n",
    "        for idx, resume_text in enumerate(resume_texts):\n",
    "            # Ask questions about each individual resume (resume_text)\n",
    "            result = qa_chain.run(f\"Summarize the following resume with focus on relevant skills for this job description: {job_description}. Resume: {resume_text[:500]}...\")  # Limit to first 1000 chars for brevity\n",
    "            results.append({\"Filename\": filenames[idx], \"Summary\": result})\n",
    "\n",
    "        # Convert results to DataFrame\n",
    "        df_results = pd.DataFrame(results)\n",
    "        df_results.index = df_results.index + 1\n",
    "        st.write(f\"Top {top_n} Resumes\")\n",
    "        st.dataframe(df_results)\n",
    "\n",
    "       \n",
    "    else:\n",
    "        st.error(\"Please enter a job description and upload at least one resume.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed2871b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting resume_screen_testapp2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile resume_screen_testapp2.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "import PyPDF2\n",
    "import io\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.docstore.document import Document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv('.env')\n",
    "\n",
    "# Fetch the OpenAI API key from the environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Function to extract text from PDF files\n",
    "def extract_text_from_pdf(file):\n",
    "    text = \"\"\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to extract text from DOCX files\n",
    "def extract_text_from_docx(file):\n",
    "    return docx2txt.process(file)\n",
    "\n",
    "# Function to extract text from TXT files\n",
    "def extract_text_from_txt(file):\n",
    "    return file.read().decode(\"utf-8\")\n",
    "\n",
    "# Function to extract text from different file types\n",
    "def extract_text(uploaded_file):\n",
    "    if uploaded_file.type == \"application/pdf\":\n",
    "        return extract_text_from_pdf(uploaded_file)\n",
    "    elif uploaded_file.type == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\":\n",
    "        return extract_text_from_docx(uploaded_file)\n",
    "    elif uploaded_file.type == \"text/plain\":\n",
    "        return extract_text_from_txt(uploaded_file)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"Resume Matcher with Advanced Retrieval\")\n",
    "\n",
    "# Input for job description\n",
    "job_description = st.text_area(\"Enter Job Description\")\n",
    "\n",
    "# Upload resumes\n",
    "uploaded_files = st.file_uploader(\"Upload Resumes\", accept_multiple_files=True, type=[\"pdf\", \"docx\", \"txt\"])\n",
    "\n",
    "# Input for number of top resumes to display\n",
    "top_n = st.number_input(\"Enter the number of top resumes to display\", min_value=1, value=5, step=1)\n",
    "\n",
    "# Add a button to run the resume matching\n",
    "if st.button(\"Match Resumes\"):\n",
    "    if job_description and uploaded_files:\n",
    "        resume_texts = []\n",
    "        filenames = []\n",
    "\n",
    "        for uploaded_file in uploaded_files:\n",
    "            try:\n",
    "                file_content = extract_text(uploaded_file)\n",
    "                resume_texts.append(file_content)\n",
    "                filenames.append(uploaded_file.name)\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error processing file {uploaded_file.name}: {e}\")\n",
    "\n",
    "        # Use the SemanticChunker to split resumes into meaningful chunks\n",
    "        embeddings_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "        text_splitter = SemanticChunker(embeddings_model)  # Semantic chunker instead of token-based\n",
    "\n",
    "        # Wrap text content into Document objects for LangChain processing\n",
    "        documents = [Document(page_content=text, metadata={\"filename\": filenames[i]}) for i, text in enumerate(resume_texts)]\n",
    "        semantic_chunks = []\n",
    "        for doc in documents:\n",
    "            chunks = text_splitter.split_documents([doc])\n",
    "            semantic_chunks.extend(chunks)\n",
    "\n",
    "        # Store chunks and embeddings in Chroma for retrieval\n",
    "        vectordb = Chroma.from_documents(semantic_chunks, embeddings_model)\n",
    "\n",
    "        # Create a LangChain RetrievalQA pipeline\n",
    "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            retriever=vectordb.as_retriever(),\n",
    "            llm=llm\n",
    "        )\n",
    "\n",
    "        # Embed the job description for similarity scoring\n",
    "        job_desc_embedding = embeddings_model.embed_query(job_description)\n",
    "\n",
    "        # Generate summaries and calculate similarity scores\n",
    "        results = []\n",
    "        for idx, resume_text in enumerate(resume_texts):\n",
    "            # Get the embedding for the current resume\n",
    "            resume_embedding = embeddings_model.embed_query(resume_text)\n",
    "\n",
    "            # Calculate cosine similarity between job description and resume\n",
    "            similarity_score = cosine_similarity(\n",
    "                [job_desc_embedding], [resume_embedding]\n",
    "            )[0][0]\n",
    "\n",
    "            # Run the RAG pipeline to generate the summary for the resume\n",
    "            result = qa_chain.run(f\"Summarize the following resume with focus on relevant skills for this job description: {job_description}. Resume: {resume_text[:1000]}...\")\n",
    "            \n",
    "            # Append the summary and similarity score to the results\n",
    "            results.append({\n",
    "                \"Filename\": filenames[idx], \n",
    "                \"Summary\": result,\n",
    "                \"Similarity Score\": similarity_score\n",
    "            })\n",
    "\n",
    "        # Sort results by similarity score in descending order\n",
    "        results = sorted(results, key=lambda x: x[\"Similarity Score\"], reverse=True)\n",
    "\n",
    "        # Display the results in a DataFrame\n",
    "        df_results = pd.DataFrame(results)\n",
    "        df_results.index = df_results.index + 1\n",
    "        st.write(f\"Top {top_n} Resumes\")\n",
    "        st.dataframe(df_results)\n",
    "\n",
    "    else:\n",
    "        st.error(\"Please enter a job description and upload at least one resume.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69dcde56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting resume_screen_testapp2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile resume_screen_testapp2.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "import PyPDF2\n",
    "import json\n",
    "import io\n",
    "import re\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.docstore.document import Document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv('.env')\n",
    "\n",
    "# Fetch the OpenAI API key from the environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Function to extract text from PDF files\n",
    "def extract_text_from_pdf(file):\n",
    "    text = \"\"\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to extract text from DOCX files\n",
    "def extract_text_from_docx(file):\n",
    "    return docx2txt.process(file)\n",
    "\n",
    "# Function to extract text from TXT files\n",
    "def extract_text_from_txt(file):\n",
    "    return file.read().decode(\"utf-8\")\n",
    "\n",
    "# Function to extract text from different file types\n",
    "def extract_text(uploaded_file):\n",
    "    if uploaded_file.type == \"application/pdf\":\n",
    "        return extract_text_from_pdf(uploaded_file)\n",
    "    elif uploaded_file.type == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\":\n",
    "        return extract_text_from_docx(uploaded_file)\n",
    "    elif uploaded_file.type == \"text/plain\":\n",
    "        return extract_text_from_txt(uploaded_file)\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"Resume Matcher with Advanced Retrieval\")\n",
    "\n",
    "# Input for job description\n",
    "job_description = st.text_area(\"Enter Job Description\")\n",
    "\n",
    "# Upload resumes\n",
    "uploaded_files = st.file_uploader(\"Upload Resumes\", accept_multiple_files=True, type=[\"pdf\", \"docx\", \"txt\"])\n",
    "\n",
    "# Input for number of top resumes to display\n",
    "top_n = st.number_input(\"Enter the number of top resumes to display\", min_value=1, value=5, step=1)\n",
    "\n",
    "# Add a button to run the resume matching\n",
    "if st.button(\"Match Resumes\"):\n",
    "    if job_description and uploaded_files:\n",
    "        resume_texts = []\n",
    "        filenames = []\n",
    "\n",
    "        for uploaded_file in uploaded_files:\n",
    "            try:\n",
    "                file_content = extract_text(uploaded_file)\n",
    "                resume_texts.append(file_content)\n",
    "                filenames.append(uploaded_file.name)\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error processing file {uploaded_file.name}: {e}\")\n",
    "\n",
    "        # Use the SemanticChunker to split resumes into meaningful chunks\n",
    "        #embeddings_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "        embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "        text_splitter = SemanticChunker(embeddings_model)  # Semantic chunker instead of token-based\n",
    "\n",
    "        # Wrap text content into Document objects for LangChain processing\n",
    "        documents = [Document(page_content=text, metadata={\"filename\": filenames[i]}) for i, text in enumerate(resume_texts)]\n",
    "        semantic_chunks = []\n",
    "        for doc in documents:\n",
    "            chunks = text_splitter.split_documents([doc])\n",
    "            semantic_chunks.extend(chunks)\n",
    "\n",
    "        # Store chunks and embeddings in Chroma for retrieval\n",
    "        vectordb = Chroma.from_documents(semantic_chunks, embeddings_model)\n",
    "\n",
    "        # Create a LangChain RetrievalQA pipeline\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            retriever=vectordb.as_retriever(),\n",
    "            llm=llm\n",
    "        )\n",
    "\n",
    "        # Embed the job description for similarity scoring\n",
    "        job_desc_embedding = embeddings_model.embed_query(job_description)\n",
    "\n",
    "\n",
    "        # Generate summaries and calculate similarity scores\n",
    "        results = []\n",
    "        for idx, resume_text in enumerate(resume_texts):\n",
    "            # Get the embedding for the current resume\n",
    "            resume_embedding = embeddings_model.embed_query(resume_text)\n",
    "\n",
    "            # Calculate cosine similarity between job description and resume\n",
    "            similarity_score = cosine_similarity(\n",
    "                [job_desc_embedding], [resume_embedding]\n",
    "            )[0][0]\n",
    "\n",
    "         # Modify the prompt to request JSON-only output\n",
    "        result = qa_chain.run(f\"\"\"\n",
    "        For the following resume, extract and return the following details in JSON format only. Do not include any additional text or explanationsâ€”just the JSON data.\n",
    "\n",
    "        Required details:\n",
    "        - Name\n",
    "        - Area of study\n",
    "        - Summary\n",
    "        - Work experiences (list of job experiences), where each job experience includes:\n",
    "            - Job title\n",
    "            - Company name\n",
    "            - Start date\n",
    "            - End date\n",
    "\n",
    "        Example output:\n",
    "        {{\n",
    "            \"Name\": \"John Doe\",\n",
    "            \"Area of study\": \"Computer Science\",\n",
    "            \"Summary\": \"Experienced software engineer with expertise in Python and machine learning.\",\n",
    "            \"Work experiences\": [\n",
    "                {{\"Job title\": \"Software Engineer\", \"Company name\": \"ABC Corp\", \"Start date\": \"Jan 2020\", \"End date\": \"Present\"}},\n",
    "                {{\"Job title\": \"Junior Developer\", \"Company name\": \"XYZ Ltd\", \"Start date\": \"May 2017\", \"End date\": \"Dec 2019\"}}\n",
    "            ]\n",
    "        }}\n",
    "        Return only the JSON data. Do not include any additional text.\n",
    "\n",
    "        Resume: {resume_text[:1000]}...\n",
    "        \"\"\")\n",
    "\n",
    "        # Try parsing the result as JSON\n",
    " \n",
    "\n",
    "        try:\n",
    "            # Extract the JSON object from the result\n",
    "            json_match = re.search(r'\\{.*\\}', result, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group()\n",
    "                parsed_result = json.loads(json_str)\n",
    "            else:\n",
    "                raise ValueError(\"No JSON object found in the model's output.\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"Failed to parse result for {filenames[idx]}: {e}\")\n",
    "            st.write(\"Model's raw output:\")\n",
    "            st.write(result)\n",
    "            parsed_result = {}\n",
    "\n",
    "        name = parsed_result.get('Name', 'N/A')\n",
    "        area_of_study = parsed_result.get('Area of study', 'N/A')\n",
    "        skills_summary = parsed_result.get('Summary', 'N/A')\n",
    "        job_experiences = parsed_result.get('Work experiences', [])\n",
    "\n",
    "        # Proceed with processing job_experiences as before\n",
    "        formatted_roles = []\n",
    "        for job in job_experiences:\n",
    "            job_title = job.get('Job title', 'N/A')\n",
    "            company = job.get('Company name', 'N/A')\n",
    "            start_date = job.get('Start date', 'N/A')\n",
    "            end_date = job.get('End date', 'N/A')\n",
    "            role_text = f\"{job_title} in {company} ({start_date} - {end_date})\"\n",
    "\n",
    "            # Calculate relevance score for this specific job role\n",
    "            role_embedding = embeddings_model.embed_query(role_text)\n",
    "            relevance_score = cosine_similarity(\n",
    "                [job_desc_embedding], [role_embedding]\n",
    "            )[0][0] * 10  # Scale the relevance score (0-10 scale)\n",
    "\n",
    "            # Format the role with relevance score\n",
    "            formatted_roles.append(f\"{job_title} in {company} [Relevant score: {relevance_score:.1f}], {start_date} - {end_date}\")\n",
    "\n",
    "        # Append the summary, similarity score, and relevant roles to the results\n",
    "        results.append({\n",
    "            \"Filename\": filenames[idx],\n",
    "            \"Name\": name,\n",
    "            \"Area of Study\": area_of_study,\n",
    "            \"Relevant Years of Experience\": '\\n'.join(formatted_roles),\n",
    "            \"Summary\": skills_summary,\n",
    "            \"Similarity Score\": similarity_score\n",
    "        })\n",
    "\n",
    "        # Sort results by similarity score in descending order\n",
    "        results = sorted(results, key=lambda x: x[\"Similarity Score\"], reverse=True)\n",
    "\n",
    "        # Display the results in a DataFrame\n",
    "        df_results = pd.DataFrame(results)\n",
    "        df_results.index = df_results.index + 1\n",
    "        st.write(f\"Top {top_n} Resumes\")\n",
    "        st.dataframe(df_results)\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        st.error(\"Please enter a job description and upload at least one resume.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7a5c1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting resume_screen_testapp2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile resume_screen_testapp2.py \n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "import PyPDF2\n",
    "import json\n",
    "import io\n",
    "import re\n",
    "import openai\n",
    "import os\n",
    "import tempfile\n",
    "import base64\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.docstore.document import Document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv('.env')\n",
    "\n",
    "# Fetch the OpenAI API key from the environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Function to extract text from PDF files\n",
    "def extract_text_from_pdf(file):\n",
    "    text = \"\"\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to extract text from DOCX files\n",
    "def extract_text_from_docx(file):\n",
    "    return docx2txt.process(file)\n",
    "\n",
    "# Function to extract text from TXT files\n",
    "def extract_text_from_txt(file):\n",
    "    return file.read().decode(\"utf-8\")\n",
    "\n",
    "# Function to extract text from different file types\n",
    "def extract_text(uploaded_file):\n",
    "    if uploaded_file.type == \"application/pdf\":\n",
    "        return extract_text_from_pdf(uploaded_file)\n",
    "    elif uploaded_file.type == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\":\n",
    "        return extract_text_from_docx(uploaded_file)\n",
    "    elif uploaded_file.type == \"text/plain\":\n",
    "        return extract_text_from_txt(uploaded_file)\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"Resume Matcher with Advanced Retrieval\")\n",
    "\n",
    "# Input for job description\n",
    "job_description = st.text_area(\"Enter Job Description\")\n",
    "\n",
    "# Upload resumes\n",
    "uploaded_files = st.file_uploader(\"Upload Resumes\", accept_multiple_files=True, type=[\"pdf\", \"docx\", \"txt\"])\n",
    "\n",
    "# Input for number of top resumes to display\n",
    "top_n = st.number_input(\"Enter the number of top resumes to display\", min_value=1, value=5, step=1)\n",
    "\n",
    "# Add a button to run the resume matching\n",
    "if st.button(\"Match Resumes\"):\n",
    "    if job_description and uploaded_files:\n",
    "        resume_texts = []\n",
    "        filenames = []\n",
    "        file_links = []  # List to store file links\n",
    "\n",
    "        for uploaded_file in uploaded_files:\n",
    "            try:\n",
    "                file_content = extract_text(uploaded_file)\n",
    "                resume_texts.append(file_content)\n",
    "                filenames.append(uploaded_file.name)\n",
    "\n",
    "                # Save the file to a temporary location\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:\n",
    "                    tmp_file.write(uploaded_file.getbuffer())\n",
    "                    tmp_file_path = tmp_file.name\n",
    "\n",
    "                # Generate a download link for the file\n",
    "                with open(tmp_file_path, 'rb') as f:\n",
    "                    file_bytes = f.read()\n",
    "\n",
    "                # Create a download link and store the HTML\n",
    "                b64 = base64.b64encode(file_bytes).decode()\n",
    "                href = f'<a href=\"data:file/{uploaded_file.type};base64,{b64}\" download=\"{uploaded_file.name}\">{uploaded_file.name}</a>'\n",
    "                file_links.append(href)\n",
    "\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error processing file {uploaded_file.name}: {e}\")\n",
    "\n",
    "        # Use the SemanticChunker to split resumes into meaningful chunks\n",
    "        embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "        text_splitter = SemanticChunker(embeddings_model)  # Semantic chunker instead of token-based\n",
    "\n",
    "        # Wrap text content into Document objects for LangChain processing\n",
    "        documents = [Document(page_content=text, metadata={\"filename\": filenames[i]}) for i, text in enumerate(resume_texts)]\n",
    "        semantic_chunks = []\n",
    "        for doc in documents:\n",
    "            chunks = text_splitter.split_documents([doc])\n",
    "            semantic_chunks.extend(chunks)\n",
    "\n",
    "        # Store chunks and embeddings in Chroma for retrieval\n",
    "        vectordb = Chroma.from_documents(semantic_chunks, embeddings_model)\n",
    "\n",
    "        # Create a LangChain RetrievalQA pipeline\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            retriever=vectordb.as_retriever(),\n",
    "            llm=llm\n",
    "        )\n",
    "\n",
    "        # Embed the job description for similarity scoring\n",
    "        job_desc_embedding = embeddings_model.embed_query(job_description)\n",
    "\n",
    "        # Generate summaries and calculate similarity scores\n",
    "        results = []\n",
    "        for idx, resume_text in enumerate(resume_texts):\n",
    "            # Get the embedding for the current resume\n",
    "            resume_embedding = embeddings_model.embed_query(resume_text)\n",
    "\n",
    "            # Calculate cosine similarity between job description and resume\n",
    "            similarity_score = cosine_similarity(\n",
    "                [job_desc_embedding], [resume_embedding]\n",
    "            )[0][0]\n",
    "\n",
    "            # Modify the prompt to request JSON-only output\n",
    "            result = qa_chain.run(f\"\"\"\n",
    "            For the following resume, extract and return the following details in JSON format only. Do not include any additional text or explanationsâ€”just the JSON data.\n",
    "\n",
    "            Required details:\n",
    "            - Name\n",
    "            - Area of study\n",
    "            - Summary\n",
    "            - Work experiences (list of job experiences), where each job experience includes:\n",
    "                - Job title\n",
    "                - Company name\n",
    "                - Start date\n",
    "                - End date\n",
    "\n",
    "            Example output:\n",
    "            {{\n",
    "                \"Name\": \"John Doe\",\n",
    "                \"Area of study\": \"Bachelor in Computer Science\",\n",
    "                \"Summary\": \"Experienced software engineer with expertise in Python and machine learning.\",\n",
    "                \"Work experiences\": [\n",
    "                    {{\"Job title\": \"Software Engineer\", \"Company name\": \"ABC Corp\", \"Start date\": \"Jan 2020\", \"End date\": \"Present\"}},\n",
    "                    {{\"Job title\": \"Junior Developer\", \"Company name\": \"XYZ Ltd\", \"Start date\": \"May 2017\", \"End date\": \"Dec 2019\"}}\n",
    "                ]\n",
    "            }}\n",
    "            Return only the JSON data. Do not include any additional text.\n",
    "\n",
    "            Resume: {resume_text[:1000]}...\n",
    "            \"\"\")\n",
    "\n",
    "            # Try parsing the result as JSON\n",
    "            try:\n",
    "                # Extract the JSON object from the result\n",
    "                json_match = re.search(r'\\{.*\\}', result, re.DOTALL)\n",
    "                if json_match:\n",
    "                    json_str = json_match.group()\n",
    "                    parsed_result = json.loads(json_str)\n",
    "                else:\n",
    "                    raise ValueError(\"No JSON object found in the model's output.\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Failed to parse result for {filenames[idx]}: {e}\")\n",
    "                st.write(\"Model's raw output:\")\n",
    "                st.write(result)\n",
    "                parsed_result = {}\n",
    "\n",
    "            name = parsed_result.get('Name', 'N/A')\n",
    "            area_of_study = parsed_result.get('Area of study', 'N/A')\n",
    "            skills_summary = parsed_result.get('Summary', 'N/A')\n",
    "            job_experiences = parsed_result.get('Work experiences', [])\n",
    "\n",
    "            # Proceed with processing job_experiences as before\n",
    "            formatted_roles = []\n",
    "            for job in job_experiences:\n",
    "                job_title = job.get('Job title', 'N/A')\n",
    "                company = job.get('Company name', 'N/A')\n",
    "                start_date = job.get('Start date', 'N/A')\n",
    "                end_date = job.get('End date', 'N/A')\n",
    "                role_text = f\"{job_title} in {company} ({start_date} - {end_date})\"\n",
    "\n",
    "                # Calculate relevance score for this specific job role\n",
    "                role_embedding = embeddings_model.embed_query(role_text)\n",
    "                relevance_score = cosine_similarity(\n",
    "                    [job_desc_embedding], [role_embedding]\n",
    "                )[0][0] * 10  # Scale the relevance score (0-10 scale)\n",
    "\n",
    "                # Format the role with relevance score\n",
    "                formatted_roles.append(f\"{job_title} in {company} [Relevant score: {relevance_score:.1f}], {start_date} - {end_date}\")\n",
    "\n",
    "            # Append the summary, similarity score, and relevant roles to the results\n",
    "            results.append({\n",
    "                \"Filename\": file_links[idx],\n",
    "                \"Name\": name,\n",
    "                \"Area of Study\": area_of_study,\n",
    "                \"Relevant Years of Experience\": '<br>'.join(formatted_roles),\n",
    "                \"Summary\": skills_summary,\n",
    "                \"Similarity Score\": similarity_score\n",
    "            })\n",
    "\n",
    "        # Sort results by similarity score in descending order\n",
    "        results = sorted(results, key=lambda x: x[\"Similarity Score\"], reverse=True)\n",
    "\n",
    "        # Display the results in a DataFrame\n",
    "        df_results = pd.DataFrame(results)\n",
    "        df_results.index = df_results.index + 1\n",
    "        st.write(f\"Top {top_n} Resumes\")\n",
    "\n",
    "        # Adjust pandas display options\n",
    "        pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "        # Apply styling to adjust cell height and allow HTML content\n",
    "        styled_df = df_results.style.set_properties(**{\n",
    "            'white-space': 'pre-wrap',\n",
    "            'word-wrap': 'break-word',\n",
    "            'text-align': 'left',\n",
    "            'vertical-align': 'top'\n",
    "        })\n",
    "\n",
    "        # Render the DataFrame with clickable links\n",
    "        st.write(styled_df.to_html(escape=False, index=False), unsafe_allow_html=True)\n",
    "\n",
    "    else:\n",
    "        st.error(\"Please enter a job description and upload at least one resume.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "85d81698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting resume_screen_testapp2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile resume_screen_testapp2.py \n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "import PyPDF2\n",
    "import json\n",
    "import io\n",
    "import re\n",
    "import openai\n",
    "import os\n",
    "import tempfile\n",
    "import base64\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.docstore.document import Document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv('.env')\n",
    "\n",
    "# Fetch the OpenAI API key and email credentials from the environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "sender_email = os.getenv('SENDER_EMAIL')\n",
    "sender_password = os.getenv('SENDER_PASSWORD')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Function to extract text from PDF files\n",
    "def extract_text_from_pdf(file):\n",
    "    text = \"\"\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to extract text from DOCX files\n",
    "def extract_text_from_docx(file):\n",
    "    return docx2txt.process(file)\n",
    "\n",
    "# Function to extract text from TXT files\n",
    "def extract_text_from_txt(file):\n",
    "    return file.read().decode(\"utf-8\")\n",
    "\n",
    "# Function to extract text from different file types\n",
    "def extract_text(uploaded_file):\n",
    "    if uploaded_file.type == \"application/pdf\":\n",
    "        return extract_text_from_pdf(uploaded_file)\n",
    "    elif uploaded_file.type == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\":\n",
    "        return extract_text_from_docx(uploaded_file)\n",
    "    elif uploaded_file.type == \"text/plain\":\n",
    "        return extract_text_from_txt(uploaded_file)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Function to send email\n",
    "def send_email(sender_email, sender_password, recipient_email, subject, message):\n",
    "    # Create a multipart message\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = sender_email\n",
    "    msg['To'] = recipient_email\n",
    "    msg['Subject'] = subject\n",
    "\n",
    "    # Add message body\n",
    "    msg.attach(MIMEText(message, 'plain'))\n",
    "\n",
    "    try:\n",
    "        # Connect to Gmail's SMTP server\n",
    "        server = smtplib.SMTP_SSL('smtp.gmail.com', 465)\n",
    "        server.login(sender_email, sender_password)\n",
    "\n",
    "        # Send the email\n",
    "        server.send_message(msg)\n",
    "        server.quit()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending email: {e}\")\n",
    "        return False\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"Resume Matcher with Advanced Retrieval\")\n",
    "\n",
    "# Input for job description\n",
    "job_description = st.text_area(\"Enter Job Description\")\n",
    "\n",
    "# Upload resumes\n",
    "uploaded_files = st.file_uploader(\"Upload Resumes\", accept_multiple_files=True, type=[\"pdf\", \"docx\", \"txt\"])\n",
    "\n",
    "# Input for number of top resumes to display\n",
    "top_n = st.number_input(\"Enter the number of top resumes to display\", min_value=1, value=5, step=1)\n",
    "\n",
    "# Add a button to run the resume matching\n",
    "if st.button(\"Match Resumes\"):\n",
    "    if job_description and uploaded_files:\n",
    "        resume_texts = []\n",
    "        filenames = []\n",
    "        resume_files = []  # To store the original file bytes\n",
    "\n",
    "        for uploaded_file in uploaded_files:\n",
    "            try:\n",
    "                file_content = extract_text(uploaded_file)\n",
    "                resume_texts.append(file_content)\n",
    "                filenames.append(uploaded_file.name)\n",
    "                resume_files.append(uploaded_file.getvalue())  # Store original file bytes\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error processing file {uploaded_file.name}: {e}\")\n",
    "\n",
    "        # Use the SemanticChunker to split resumes into meaningful chunks\n",
    "        embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "        text_splitter = SemanticChunker(embeddings_model)  # Semantic chunker instead of token-based\n",
    "\n",
    "        # Wrap text content into Document objects for LangChain processing\n",
    "        documents = [Document(page_content=text, metadata={\"filename\": filenames[i]}) for i, text in enumerate(resume_texts)]\n",
    "        semantic_chunks = []\n",
    "        for doc in documents:\n",
    "            chunks = text_splitter.split_documents([doc])\n",
    "            semantic_chunks.extend(chunks)\n",
    "\n",
    "        # Store chunks and embeddings in Chroma for retrieval\n",
    "        vectordb = Chroma.from_documents(semantic_chunks, embeddings_model)\n",
    "\n",
    "        # Create a LangChain RetrievalQA pipeline\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            retriever=vectordb.as_retriever(),\n",
    "            llm=llm\n",
    "        )\n",
    "\n",
    "        # Embed the job description for similarity scoring\n",
    "        job_desc_embedding = embeddings_model.embed_query(job_description)\n",
    "\n",
    "        # Generate summaries and calculate similarity scores\n",
    "        results = []\n",
    "        for idx, resume_text in enumerate(resume_texts):\n",
    "            # Get the embedding for the current resume\n",
    "            resume_embedding = embeddings_model.embed_query(resume_text)\n",
    "\n",
    "            # Calculate cosine similarity between job description and resume\n",
    "            similarity_score = cosine_similarity(\n",
    "                [job_desc_embedding], [resume_embedding]\n",
    "            )[0][0]\n",
    "\n",
    "            # Modify the prompt to request JSON-only output\n",
    "            result = qa_chain.run(f\"\"\"\n",
    "            For the following resume, extract and return the following details in JSON format only. Do not include any additional text or explanationsâ€”just the JSON data.\n",
    "\n",
    "            Required details:\n",
    "            - Name\n",
    "            - Education (list all tertiary degrees and above obtained, including degree name and field of study. If no tertiary degree then state the highest education achieved)\n",
    "            - Highlights of Career (2-3 standout sentences from the resume)\n",
    "\n",
    "            - Work experiences (list of job experiences), where each job experience includes:\n",
    "                - Job title\n",
    "                - Company name\n",
    "                - Start date\n",
    "                - End date\n",
    "\n",
    "            Example output:\n",
    "            {{\n",
    "                \"Name\": \"John Doe\",\n",
    "                \"Education\": [\n",
    "                    \"Bachelor in Computer Science\",\n",
    "                    \"Master of Business Administration\"\n",
    "                ],\n",
    "                \"Highlights of Career\": [\n",
    "                    \"Led a team of developers to create a cutting-edge AI application.\",\n",
    "                    \"Increased company revenue by 20% through innovative solutions.\"\n",
    "                ],\n",
    "                \"Work experiences\": [\n",
    "                    {{\"Job title\": \"Software Engineer\", \"Company name\": \"ABC Corp\", \"Start date\": \"Jan 2020\", \"End date\": \"Present\"}},\n",
    "                    {{\"Job title\": \"Junior Developer\", \"Company name\": \"XYZ Ltd\", \"Start date\": \"May 2017\", \"End date\": \"Dec 2019\"}}\n",
    "                ]\n",
    "            }}\n",
    "            Return only the JSON data. Do not include any additional text.\n",
    "\n",
    "            Resume: {resume_text[:1000]}...\n",
    "            \"\"\")\n",
    "\n",
    "            # Try parsing the result as JSON\n",
    "            try:\n",
    "                # Extract the JSON object from the result\n",
    "                json_match = re.search(r'\\{.*\\}', result, re.DOTALL)\n",
    "                if json_match:\n",
    "                    json_str = json_match.group()\n",
    "                    parsed_result = json.loads(json_str)\n",
    "                else:\n",
    "                    raise ValueError(\"No JSON object found in the model's output.\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Failed to parse result for {filenames[idx]}: {e}\")\n",
    "                st.write(\"Model's raw output:\")\n",
    "                st.write(result)\n",
    "                parsed_result = {}\n",
    "\n",
    "            name = parsed_result.get('Name', 'N/A')\n",
    "            education = parsed_result.get('Education', [])\n",
    "            highlights = parsed_result.get('Highlights of Career', [])\n",
    "            job_experiences = parsed_result.get('Work experiences', [])\n",
    "\n",
    "            # Proceed with processing job_experiences as before\n",
    "            formatted_roles = []\n",
    "            for job in job_experiences:\n",
    "                job_title = job.get('Job title', 'N/A')\n",
    "                company = job.get('Company name', 'N/A')\n",
    "                start_date = job.get('Start date', 'N/A')\n",
    "                end_date = job.get('End date', 'N/A')\n",
    "                role_text = f\"{job_title} in {company} ({start_date} - {end_date})\"\n",
    "\n",
    "                # Calculate relevance score for this specific job role\n",
    "                role_embedding = embeddings_model.embed_query(role_text)\n",
    "                relevance_score = cosine_similarity(\n",
    "                    [job_desc_embedding], [role_embedding]\n",
    "                )[0][0] * 10  # Scale the relevance score (0-10 scale)\n",
    "\n",
    "                # Format the role with relevance score\n",
    "                formatted_roles.append(f\"{job_title} in {company} [Relevant score: {relevance_score:.1f}], {start_date} - {end_date}\")\n",
    "\n",
    "            # Append the summary, similarity score, and relevant roles to the results\n",
    "            results.append({\n",
    "                \"Filename\": filenames[idx],\n",
    "                \"Name\": name,\n",
    "                \"Education\": education,\n",
    "                \"Highlights\": highlights,\n",
    "                \"Relevant Years of Experience\": '<br>'.join(formatted_roles),\n",
    "                \"Similarity Score\": similarity_score,\n",
    "                \"Resume Text\": resume_text  # Include the resume text\n",
    "            })\n",
    "\n",
    "        # Sort results by similarity score in descending order\n",
    "        results = sorted(results, key=lambda x: x[\"Similarity Score\"], reverse=True)\n",
    "\n",
    "        # Display the results\n",
    "        st.write(f\"Top {top_n} Resumes\")\n",
    "\n",
    "        for idx, result in enumerate(results[:top_n]):\n",
    "            st.subheader(f\"{idx + 1}. {result['Name']} ({result['Filename']})\")\n",
    "            st.write(f\"**Similarity Score:** {result['Similarity Score']:.2f}\")\n",
    "\n",
    "            # Display Education\n",
    "            st.write(\"**Education:**\")\n",
    "            education = result.get('Education', [])\n",
    "            if education:\n",
    "                for edu in education:\n",
    "                    st.write(f\"- {edu}\")\n",
    "            else:\n",
    "                st.write(\"No education details available.\")\n",
    "\n",
    "            # Display Highlights of Career\n",
    "            st.write(\"**Highlights of Career:**\")\n",
    "            highlights = result.get('Highlights', [])\n",
    "            if highlights:\n",
    "                for highlight in highlights:\n",
    "                    st.write(f\"- {highlight}\")\n",
    "            else:\n",
    "                st.write(\"No highlights available.\")\n",
    "\n",
    "            st.write(\"**Relevant Years of Experience:**\", unsafe_allow_html=True)\n",
    "            st.write(result['Relevant Years of Experience'], unsafe_allow_html=True)\n",
    "\n",
    "            # Add an expander to show the resume content\n",
    "            with st.expander(\"Show Resume Content\"):\n",
    "                st.write(result['Resume Text'])\n",
    "\n",
    "            # Email Sending Section\n",
    "            st.write(\"**Forward Summary to Hiring Manager:**\")\n",
    "            recipient_email = st.text_input(f\"Enter hiring manager's email for {result['Name']}:\", key=f\"email_{idx}\")\n",
    "            if st.button(f\"Send Summary for {result['Name']}\", key=f\"send_{idx}\"):\n",
    "                if recipient_email:\n",
    "                    # Prepare the email content\n",
    "                    subject = f\"Resume Summary for {result['Name']}\"\n",
    "                    message = f\"\"\"\n",
    "Name: {result['Name']}\n",
    "Similarity Score: {result['Similarity Score']:.2f}\n",
    "\n",
    "Education:\n",
    "\"\"\"\n",
    "                    education = result.get('Education', [])\n",
    "                    if education:\n",
    "                        for edu in education:\n",
    "                            message += f\"- {edu}\\n\"\n",
    "                    else:\n",
    "                        message += \"No education details available.\\n\"\n",
    "\n",
    "                    message += \"\\nHighlights of Career:\\n\"\n",
    "                    highlights = result.get('Highlights', [])\n",
    "                    if highlights:\n",
    "                        for highlight in highlights:\n",
    "                            message += f\"- {highlight}\\n\"\n",
    "                    else:\n",
    "                        message += \"No highlights available.\\n\"\n",
    "\n",
    "                    message += \"\\nRelevant Years of Experience:\\n\"\n",
    "                    message += result['Relevant Years of Experience'].replace('<br>', '\\n')\n",
    "\n",
    "                    # Send the email\n",
    "                    success = send_email(sender_email, sender_password, recipient_email, subject, message)\n",
    "                    if success:\n",
    "                        st.success(f\"Email sent to {recipient_email}\")\n",
    "                    else:\n",
    "                        st.error(f\"Failed to send email to {recipient_email}\")\n",
    "                else:\n",
    "                    st.error(\"Please enter a recipient email address.\")\n",
    "\n",
    "            st.markdown(\"---\")  # Separator between resumes\n",
    "\n",
    "    else:\n",
    "        st.error(\"Please enter a job description and upload at least one resume.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "695a447c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#summary not in table but display properly\n",
    "!streamlit run resume_screen_testapp2.py --server.enableXsrfProtection false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10fbaba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting resume_screen_testapp2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile resume_screen_testapp2.py \n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "import PyPDF2\n",
    "import json\n",
    "import io\n",
    "import re\n",
    "import openai\n",
    "import os\n",
    "import tempfile\n",
    "import base64\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.docstore.document import Document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv('.env')\n",
    "\n",
    "# Fetch the OpenAI API key and email credentials from the environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "sender_email = os.getenv('SENDER_EMAIL')\n",
    "sender_password = os.getenv('SENDER_PASSWORD')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Function to extract text from PDF files\n",
    "def extract_text_from_pdf(file):\n",
    "    text = \"\"\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to extract text from DOCX files\n",
    "def extract_text_from_docx(file):\n",
    "    return docx2txt.process(file)\n",
    "\n",
    "# Function to extract text from TXT files\n",
    "def extract_text_from_txt(file):\n",
    "    return file.read().decode(\"utf-8\")\n",
    "\n",
    "# Function to extract text from different file types\n",
    "def extract_text(uploaded_file):\n",
    "    if uploaded_file.type == \"application/pdf\":\n",
    "        return extract_text_from_pdf(uploaded_file)\n",
    "    elif uploaded_file.type == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\":\n",
    "        return extract_text_from_docx(uploaded_file)\n",
    "    elif uploaded_file.type == \"text/plain\":\n",
    "        return extract_text_from_txt(uploaded_file)\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "# Function to send email\n",
    "def send_email(sender_email, sender_password, recipient_email, subject, message, content_type='plain'):\n",
    "    # Create a multipart message\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = sender_email\n",
    "    msg['To'] = recipient_email\n",
    "    msg['Subject'] = subject\n",
    "\n",
    "    # Add message body\n",
    "    msg.attach(MIMEText(message, content_type))\n",
    "\n",
    "    try:\n",
    "        # Connect to Gmail's SMTP server\n",
    "        server = smtplib.SMTP_SSL('smtp.gmail.com', 465)\n",
    "        server.login(sender_email, sender_password)\n",
    "\n",
    "        # Send the email\n",
    "        server.send_message(msg)\n",
    "        server.quit()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending email: {e}\")\n",
    "        return False\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"Resume Matcher with Advanced Retrieval\")\n",
    "\n",
    "# Input for job description\n",
    "job_description = st.text_area(\"Enter Job Description\")\n",
    "\n",
    "# Upload resumes\n",
    "uploaded_files = st.file_uploader(\"Upload Resumes\", accept_multiple_files=True, type=[\"pdf\", \"docx\", \"txt\"])\n",
    "\n",
    "# Input for number of top resumes to display\n",
    "top_n = st.number_input(\"Enter the number of top resumes to display\", min_value=1, value=5, step=1)\n",
    "\n",
    "# Add a button to run the resume matching\n",
    "if st.button(\"Match Resumes\"):\n",
    "    if job_description and uploaded_files:\n",
    "        resume_texts = []\n",
    "        filenames = []\n",
    "        resume_files = []  # To store the original file bytes\n",
    "\n",
    "        for uploaded_file in uploaded_files:\n",
    "            try:\n",
    "                file_content = extract_text(uploaded_file)\n",
    "                resume_texts.append(file_content)\n",
    "                filenames.append(uploaded_file.name)\n",
    "                resume_files.append(uploaded_file.getvalue())  # Store original file bytes\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error processing file {uploaded_file.name}: {e}\")\n",
    "\n",
    "        # Use the SemanticChunker to split resumes into meaningful chunks\n",
    "        embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "        text_splitter = SemanticChunker(embeddings_model)  # Semantic chunker instead of token-based\n",
    "\n",
    "        # Wrap text content into Document objects for LangChain processing\n",
    "        documents = [Document(page_content=text, metadata={\"filename\": filenames[i]}) for i, text in enumerate(resume_texts)]\n",
    "        semantic_chunks = []\n",
    "        for doc in documents:\n",
    "            chunks = text_splitter.split_documents([doc])\n",
    "            semantic_chunks.extend(chunks)\n",
    "\n",
    "        # Store chunks and embeddings in Chroma for retrieval\n",
    "        vectordb = Chroma.from_documents(semantic_chunks, embeddings_model)\n",
    "\n",
    "        # Create a LangChain RetrievalQA pipeline\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            retriever=vectordb.as_retriever(),\n",
    "            llm=llm\n",
    "        )\n",
    "\n",
    "        # Embed the job description for similarity scoring\n",
    "        job_desc_embedding = embeddings_model.embed_query(job_description)\n",
    "\n",
    "        # Generate summaries and calculate similarity scores\n",
    "        results = []\n",
    "        for idx, resume_text in enumerate(resume_texts):\n",
    "            # Get the embedding for the current resume\n",
    "            resume_embedding = embeddings_model.embed_query(resume_text)\n",
    "\n",
    "            # Calculate cosine similarity between job description and resume\n",
    "            similarity_score = cosine_similarity(\n",
    "                [job_desc_embedding], [resume_embedding]\n",
    "            )[0][0]\n",
    "\n",
    "            # Modify the prompt to request JSON-only output\n",
    "            result = qa_chain.run(f\"\"\"\n",
    "            For the following resume, extract and return the following details in JSON format only. Do not include any additional text or explanationsâ€”just the JSON data.\n",
    "\n",
    "            Required details:\n",
    "            - Name\n",
    "            - Education (list all tertiary degrees and above obtained, including degree name and field of study. If no tertiary degree then state the highest education achieved)\n",
    "            - Highlights of Career (2-3 standout sentences from the resume)\n",
    "\n",
    "            - Work experiences (list of job experiences), where each job experience includes:\n",
    "                - Job title\n",
    "                - Company name\n",
    "                - Start date\n",
    "                - End date\n",
    "\n",
    "            Example output:\n",
    "            {{\n",
    "                \"Name\": \"John Doe\",\n",
    "                \"Education\": [\n",
    "                    \"Bachelor in Computer Science\",\n",
    "                    \"Master of Business Administration\"\n",
    "                ],\n",
    "                \"Highlights of Career\": [\n",
    "                    \"Led a team of developers to create a cutting-edge AI application.\",\n",
    "                    \"Increased company revenue by 20% through innovative solutions.\"\n",
    "                ],\n",
    "                \"Work experiences\": [\n",
    "                    {{\"Job title\": \"Software Engineer\", \"Company name\": \"ABC Corp\", \"Start date\": \"Jan 2020\", \"End date\": \"Present\"}},\n",
    "                    {{\"Job title\": \"Junior Developer\", \"Company name\": \"XYZ Ltd\", \"Start date\": \"May 2017\", \"End date\": \"Dec 2019\"}}\n",
    "                ]\n",
    "            }}\n",
    "            Return only the JSON data. Do not include any additional text.\n",
    "\n",
    "            Resume: {resume_text[:1000]}...\n",
    "            \"\"\")\n",
    "\n",
    "            # Try parsing the result as JSON\n",
    "            try:\n",
    "                # Extract the JSON object from the result\n",
    "                json_match = re.search(r'\\{.*\\}', result, re.DOTALL)\n",
    "                if json_match:\n",
    "                    json_str = json_match.group()\n",
    "                    parsed_result = json.loads(json_str)\n",
    "                else:\n",
    "                    raise ValueError(\"No JSON object found in the model's output.\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Failed to parse result for {filenames[idx]}: {e}\")\n",
    "                st.write(\"Model's raw output:\")\n",
    "                st.write(result)\n",
    "                parsed_result = {}\n",
    "\n",
    "            name = parsed_result.get('Name', 'N/A')\n",
    "            education = parsed_result.get('Education', [])\n",
    "            highlights = parsed_result.get('Highlights of Career', [])\n",
    "            job_experiences = parsed_result.get('Work experiences', [])\n",
    "\n",
    "            # Proceed with processing job_experiences as before\n",
    "            formatted_roles = []\n",
    "            for job in job_experiences:\n",
    "                job_title = job.get('Job title', 'N/A')\n",
    "                company = job.get('Company name', 'N/A')\n",
    "                start_date = job.get('Start date', 'N/A')\n",
    "                end_date = job.get('End date', 'N/A')\n",
    "                role_text = f\"{job_title} in {company} ({start_date} - {end_date})\"\n",
    "\n",
    "                # Calculate relevance score for this specific job role\n",
    "                role_embedding = embeddings_model.embed_query(role_text)\n",
    "                relevance_score = cosine_similarity(\n",
    "                    [job_desc_embedding], [role_embedding]\n",
    "                )[0][0] * 10  # Scale the relevance score (0-10 scale)\n",
    "\n",
    "                # Format the role with relevance score\n",
    "                formatted_roles.append(f\"{job_title} in {company} [Relevant score: {relevance_score:.1f}], {start_date} - {end_date}\")\n",
    "\n",
    "            # Append the summary, similarity score, and relevant roles to the results\n",
    "            results.append({\n",
    "                \"Filename\": filenames[idx],\n",
    "                \"Name\": name,\n",
    "                \"Education\": education,\n",
    "                \"Highlights\": highlights,\n",
    "                \"Relevant Years of Experience\": '<br>'.join(formatted_roles),\n",
    "                \"Similarity Score\": similarity_score,\n",
    "                \"Resume Text\": resume_text  # Include the resume text\n",
    "            })\n",
    "\n",
    "        # Sort results by similarity score in descending order\n",
    "        results = sorted(results, key=lambda x: x[\"Similarity Score\"], reverse=True)\n",
    "\n",
    "        # Convert results to DataFrame\n",
    "        df_results = pd.DataFrame(results)\n",
    "\n",
    "        # For columns that are lists (Education, Highlights), join them into strings\n",
    "        df_results['Education'] = df_results['Education'].apply(lambda x: '; '.join(x) if isinstance(x, list) else x)\n",
    "        df_results['Highlights'] = df_results['Highlights'].apply(lambda x: '; '.join(x) if isinstance(x, list) else x)\n",
    "        df_results['Relevant Years of Experience'] = df_results['Relevant Years of Experience'].str.replace('<br>', '\\n')\n",
    "\n",
    "        # Reorder columns if necessary\n",
    "        df_results = df_results[['Filename', 'Name', 'Education', 'Highlights', 'Relevant Years of Experience', 'Similarity Score']]\n",
    "\n",
    "        # Display the DataFrame\n",
    "        st.write(f\"Top {top_n} Resumes\")\n",
    "        st.dataframe(df_results)\n",
    "\n",
    "        # Input field for hiring manager's email\n",
    "        recipient_email = st.text_input(\"Enter hiring manager's email:\")\n",
    "\n",
    "        # Button to send the summary table\n",
    "        if st.button(\"Forward Summary to Hiring Manager\"):\n",
    "            if recipient_email:\n",
    "                # Convert DataFrame to HTML\n",
    "                html_table = df_results.to_html(index=False, escape=False)\n",
    "\n",
    "                # Prepare the email content\n",
    "                subject = \"Resume Summaries\"\n",
    "                message = f\"\"\"\n",
    "                <p>Dear Hiring Manager,</p>\n",
    "                <p>Please find below the summaries of the top {top_n} resumes:</p>\n",
    "                {html_table}\n",
    "                <p>Best regards,<br>Your Name</p>\n",
    "                \"\"\"\n",
    "\n",
    "                # Send the email (need to adjust send_email function to handle HTML content)\n",
    "                success = send_email(sender_email, sender_password, recipient_email, subject, message, content_type='html')\n",
    "                if success:\n",
    "                    st.success(f\"Email sent to {recipient_email}\")\n",
    "                else:\n",
    "                    st.error(f\"Failed to send email to {recipient_email}\")\n",
    "            else:\n",
    "                st.error(\"Please enter a recipient email address.\")\n",
    "\n",
    "    else:\n",
    "        st.error(\"Please enter a job description and upload at least one resume.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "172cb289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# summary in table but single line, no link\n",
    "!streamlit run resume_screen_testapp2.py --server.enableXsrfProtection false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9cf0cae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting resume_screen_testapp2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile resume_screen_testapp2.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "import PyPDF2\n",
    "import json\n",
    "import re\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.docstore.document import Document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.application import MIMEApplication\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv('.env')\n",
    "\n",
    "# Fetch the OpenAI API key and email credentials from the environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "sender_email = os.getenv('SENDER_EMAIL')\n",
    "sender_password = os.getenv('SENDER_PASSWORD')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Function to extract text from PDF files\n",
    "def extract_text_from_pdf(file):\n",
    "    text = \"\"\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to extract text from DOCX files\n",
    "def extract_text_from_docx(file):\n",
    "    return docx2txt.process(file)\n",
    "\n",
    "# Function to extract text from TXT files\n",
    "def extract_text_from_txt(file):\n",
    "    return file.read().decode(\"utf-8\")\n",
    "\n",
    "# Function to extract text from different file types\n",
    "def extract_text(uploaded_file):\n",
    "    if uploaded_file.type == \"application/pdf\":\n",
    "        return extract_text_from_pdf(uploaded_file)\n",
    "    elif uploaded_file.type == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\":\n",
    "        return extract_text_from_docx(uploaded_file)\n",
    "    elif uploaded_file.type == \"text/plain\":\n",
    "        return extract_text_from_txt(uploaded_file)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Function to send email with attachment\n",
    "def send_email(sender_email, sender_password, recipient_email, subject, message, attachment=None, attachment_filename='attachment.csv'):\n",
    "    try:\n",
    "        # Create the email message\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = sender_email\n",
    "        msg['To'] = recipient_email\n",
    "        msg['Subject'] = subject\n",
    "\n",
    "        # Add message body\n",
    "        msg.attach(MIMEText(message, 'plain'))\n",
    "\n",
    "        # Attach the CSV file if provided\n",
    "        if attachment is not None:\n",
    "            part = MIMEApplication(attachment, Name=attachment_filename)\n",
    "            part['Content-Disposition'] = f'attachment; filename=\"{attachment_filename}\"'\n",
    "            msg.attach(part)\n",
    "\n",
    "        # Connect to SMTP server using STARTTLS\n",
    "        server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "        server.starttls()\n",
    "        server.login(sender_email, sender_password)\n",
    "\n",
    "        # Send the email\n",
    "        server.sendmail(sender_email, recipient_email, msg.as_string())\n",
    "        server.quit()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending email: {e}\")\n",
    "        return False\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"Resume Matcher with Advanced Retrieval\")\n",
    "\n",
    "# Input for job description\n",
    "job_description = st.text_area(\"Enter Job Description\")\n",
    "\n",
    "# Upload resumes\n",
    "uploaded_files = st.file_uploader(\"Upload Resumes\", accept_multiple_files=True, type=[\"pdf\", \"docx\", \"txt\"])\n",
    "\n",
    "# Input for number of top resumes to display\n",
    "top_n = st.number_input(\"Enter the number of top resumes to display\", min_value=1, value=5, step=1)\n",
    "\n",
    "# Add a button to run the resume matching\n",
    "if st.button(\"Match Resumes\"):\n",
    "    if job_description and uploaded_files:\n",
    "        resume_texts = []\n",
    "        filenames = []\n",
    "        resume_files = []  # To store the original file bytes\n",
    "\n",
    "        for uploaded_file in uploaded_files:\n",
    "            try:\n",
    "                file_content = extract_text(uploaded_file)\n",
    "                resume_texts.append(file_content)\n",
    "                filenames.append(uploaded_file.name)\n",
    "                resume_files.append(uploaded_file.getvalue())  # Store original file bytes\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error processing file {uploaded_file.name}: {e}\")\n",
    "\n",
    "        # Use the SemanticChunker to split resumes into meaningful chunks\n",
    "        embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "        text_splitter = SemanticChunker(embeddings_model)  # Semantic chunker instead of token-based\n",
    "\n",
    "        # Wrap text content into Document objects for LangChain processing\n",
    "        documents = [Document(page_content=text, metadata={\"filename\": filenames[i]}) for i, text in enumerate(resume_texts)]\n",
    "        semantic_chunks = []\n",
    "        for doc in documents:\n",
    "            chunks = text_splitter.split_documents([doc])\n",
    "            semantic_chunks.extend(chunks)\n",
    "\n",
    "        # Store chunks and embeddings in Chroma for retrieval\n",
    "        vectordb = Chroma.from_documents(semantic_chunks, embeddings_model)\n",
    "\n",
    "        # Create a LangChain RetrievalQA pipeline\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            retriever=vectordb.as_retriever(),\n",
    "            llm=llm\n",
    "        )\n",
    "\n",
    "        # Embed the job description for similarity scoring\n",
    "        job_desc_embedding = embeddings_model.embed_query(job_description)\n",
    "\n",
    "        # Generate summaries and calculate similarity scores\n",
    "        results = []\n",
    "        for idx, resume_text in enumerate(resume_texts):\n",
    "            # Get the embedding for the current resume\n",
    "            resume_embedding = embeddings_model.embed_query(resume_text)\n",
    "\n",
    "            # Calculate cosine similarity between job description and resume\n",
    "            similarity_score = cosine_similarity(\n",
    "                [job_desc_embedding], [resume_embedding]\n",
    "            )[0][0]\n",
    "\n",
    "            # Modify the prompt to request JSON-only output\n",
    "            result = qa_chain.run(f\"\"\"\n",
    "            For the following resume, extract and return the following details in JSON format only. Do not include any additional text or explanationsâ€”just the JSON data.\n",
    "\n",
    "            Required details:\n",
    "            - Name\n",
    "            - Education (list all tertiary degrees and above obtained, including degree name and field of study. If no tertiary degree then state the highest education achieved)\n",
    "            - Highlights of Career (2-3 standout sentences from the resume)\n",
    "\n",
    "            - Work experiences (list of job experiences), where each job experience includes:\n",
    "                - Job title\n",
    "                - Company name\n",
    "                - Start date\n",
    "                - End date\n",
    "\n",
    "            Example output:\n",
    "            {{\n",
    "                \"Name\": \"John Doe\",\n",
    "                \"Education\": [\n",
    "                    \"Bachelor in Computer Science\",\n",
    "                    \"Master of Business Administration\"\n",
    "                ],\n",
    "                \"Highlights of Career\": [\n",
    "                    \"Led a team of developers to create a cutting-edge AI application.\",\n",
    "                    \"Increased company revenue by 20% through innovative solutions.\"\n",
    "                ],\n",
    "                \"Work experiences\": [\n",
    "                    {{\"Job title\": \"Software Engineer\", \"Company name\": \"ABC Corp\", \"Start date\": \"Jan 2020\", \"End date\": \"Present\"}},\n",
    "                    {{\"Job title\": \"Junior Developer\", \"Company name\": \"XYZ Ltd\", \"Start date\": \"May 2017\", \"End date\": \"Dec 2019\"}}\n",
    "                ]\n",
    "            }}\n",
    "            Return only the JSON data. Do not include any additional text.\n",
    "\n",
    "            Resume: {resume_text[:1000]}...\n",
    "            \"\"\")\n",
    "\n",
    "            # Try parsing the result as JSON\n",
    "            try:\n",
    "                # Extract the JSON object from the result\n",
    "                json_match = re.search(r'\\{.*\\}', result, re.DOTALL)\n",
    "                if json_match:\n",
    "                    json_str = json_match.group()\n",
    "                    parsed_result = json.loads(json_str)\n",
    "                else:\n",
    "                    raise ValueError(\"No JSON object found in the model's output.\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Failed to parse result for {filenames[idx]}: {e}\")\n",
    "                st.write(\"Model's raw output:\")\n",
    "                st.write(result)\n",
    "                parsed_result = {}\n",
    "\n",
    "            name = parsed_result.get('Name', 'N/A')\n",
    "            education = parsed_result.get('Education', [])\n",
    "            highlights = parsed_result.get('Highlights of Career', [])\n",
    "            job_experiences = parsed_result.get('Work experiences', [])\n",
    "\n",
    "            # Proceed with processing job_experiences as before\n",
    "            formatted_roles = []\n",
    "            for job in job_experiences:\n",
    "                job_title = job.get('Job title', 'N/A')\n",
    "                company = job.get('Company name', 'N/A')\n",
    "                start_date = job.get('Start date', 'N/A')\n",
    "                end_date = job.get('End date', 'N/A')\n",
    "                role_text = f\"{job_title} in {company} ({start_date} - {end_date})\"\n",
    "\n",
    "                # Calculate relevance score for this specific job role\n",
    "                role_embedding = embeddings_model.embed_query(role_text)\n",
    "                relevance_score = cosine_similarity(\n",
    "                    [job_desc_embedding], [role_embedding]\n",
    "                )[0][0] * 10  # Scale the relevance score (0-10 scale)\n",
    "\n",
    "                # Format the role with relevance score\n",
    "                formatted_roles.append(f\"{job_title} in {company} [Relevant score: {relevance_score:.1f}], {start_date} - {end_date}\")\n",
    "\n",
    "            # Append the summary, similarity score, and relevant roles to the results\n",
    "            results.append({\n",
    "                \"Filename\": filenames[idx],\n",
    "                \"Name\": name,\n",
    "                \"Education\": education,\n",
    "                \"Highlights\": highlights,\n",
    "                \"Relevant Years of Experience\": '<br>'.join(formatted_roles),\n",
    "                \"Similarity Score\": similarity_score,\n",
    "                \"Resume Text\": resume_text  # Include the resume text\n",
    "            })\n",
    "\n",
    "        # Sort results by similarity score in descending order\n",
    "        results = sorted(results, key=lambda x: x[\"Similarity Score\"], reverse=True)\n",
    "\n",
    "        # Convert results to DataFrame\n",
    "        df_results = pd.DataFrame(results)\n",
    "\n",
    "        # For columns that are lists (Education, Highlights), join them into strings\n",
    "        df_results['Education'] = df_results['Education'].apply(lambda x: '; '.join(x) if isinstance(x, list) else x)\n",
    "        df_results['Highlights'] = df_results['Highlights'].apply(lambda x: '; '.join(x) if isinstance(x, list) else x)\n",
    "        df_results['Relevant Years of Experience'] = df_results['Relevant Years of Experience'].str.replace('<br>', '\\n')\n",
    "\n",
    "        # Reorder columns if necessary\n",
    "        df_results = df_results[['Filename', 'Name', 'Education', 'Highlights', 'Relevant Years of Experience', 'Similarity Score']]\n",
    "\n",
    "        # Modify the 'Filename' column to include links\n",
    "        for idx in df_results.index:\n",
    "            resume_idx = idx\n",
    "            df_results.at[idx, 'Filename'] = f'<a href=\"?resume_idx={resume_idx}\">{df_results.at[idx, \"Filename\"]}</a>'\n",
    "\n",
    "        # Display the DataFrame with links\n",
    "        st.write(f\"Top {top_n} Resumes\")\n",
    "        st.write(df_results.to_html(escape=False, index=False), unsafe_allow_html=True)\n",
    "\n",
    "        # Check if a resume index is present in query parameters\n",
    "        params = st.query_params()\n",
    "        if 'resume_idx' in params:\n",
    "            try:\n",
    "                resume_idx = int(params['resume_idx'][0])\n",
    "                if 0 <= resume_idx < len(resume_texts):\n",
    "                    st.write(f\"**Resume Content for {results[resume_idx]['Filename']}:**\")\n",
    "                    st.write(results[resume_idx]['Resume Text'])\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        # Input field for hiring manager's email\n",
    "        recipient_email = st.text_input(\"Enter hiring manager's email:\")\n",
    "\n",
    "        # Button to send the summary table\n",
    "        if st.button(\"Forward Summary to Hiring Manager\"):\n",
    "            if recipient_email:\n",
    "                # Convert DataFrame to CSV\n",
    "                csv_data = df_results.to_csv(index=False)\n",
    "\n",
    "                # Prepare the email content\n",
    "                subject = \"Resume Summaries\"\n",
    "                message = \"Dear Hiring Manager,\\n\\nPlease find attached the summaries of the top resumes.\\n\\nBest regards,\\nYour Name\"\n",
    "\n",
    "                # Send the email with attachment\n",
    "                success = send_email(\n",
    "                    sender_email, sender_password, recipient_email,\n",
    "                    subject, message,\n",
    "                    attachment=csv_data.encode('utf-8'),\n",
    "                    attachment_filename='resume_summaries.csv'\n",
    "                )\n",
    "                if success:\n",
    "                    st.success(f\"Email sent to {recipient_email}\")\n",
    "                else:\n",
    "                    st.error(f\"Failed to send email to {recipient_email}\")\n",
    "            else:\n",
    "                st.error(\"Please enter a recipient email address.\")\n",
    "\n",
    "    else:\n",
    "        st.error(\"Please enter a job description and upload at least one resume.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6e9ec000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#summary is tabled with proper height, but no link\n",
    "!streamlit run resume_screen_testapp2.py --server.enableXsrfProtection false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c8399d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting resume_screen_testapp2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile resume_screen_testapp2.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "import PyPDF2\n",
    "import json\n",
    "import re\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.docstore.document import Document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.application import MIMEApplication\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv('.env')\n",
    "\n",
    "# Fetch the OpenAI API key and email credentials from the environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "sender_email = os.getenv('SENDER_EMAIL')\n",
    "sender_password = os.getenv('SENDER_PASSWORD')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Function to extract text from PDF files\n",
    "def extract_text_from_pdf(file):\n",
    "    text = \"\"\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to extract text from DOCX files\n",
    "def extract_text_from_docx(file):\n",
    "    return docx2txt.process(file)\n",
    "\n",
    "# Function to extract text from TXT files\n",
    "def extract_text_from_txt(file):\n",
    "    return file.read().decode(\"utf-8\")\n",
    "\n",
    "# Function to extract text from different file types\n",
    "def extract_text(uploaded_file):\n",
    "    if uploaded_file.type == \"application/pdf\":\n",
    "        return extract_text_from_pdf(uploaded_file)\n",
    "    elif uploaded_file.type == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\":\n",
    "        return extract_text_from_docx(uploaded_file)\n",
    "    elif uploaded_file.type == \"text/plain\":\n",
    "        return extract_text_from_txt(uploaded_file)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Function to send email with attachment\n",
    "def send_email(sender_email, sender_password, recipient_email, subject, message, attachment=None, attachment_filename='attachment.csv'):\n",
    "    try:\n",
    "        # Create the email message\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = sender_email\n",
    "        msg['To'] = recipient_email\n",
    "        msg['Subject'] = subject\n",
    "\n",
    "        # Add message body\n",
    "        msg.attach(MIMEText(message, 'plain'))\n",
    "\n",
    "        # Attach the CSV file if provided\n",
    "        if attachment is not None:\n",
    "            part = MIMEApplication(attachment, Name=attachment_filename)\n",
    "            part['Content-Disposition'] = f'attachment; filename=\"{attachment_filename}\"'\n",
    "            msg.attach(part)\n",
    "\n",
    "        # Connect to SMTP server using STARTTLS\n",
    "        server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "        server.starttls()\n",
    "        server.login(sender_email, sender_password)\n",
    "\n",
    "        # Send the email\n",
    "        server.sendmail(sender_email, recipient_email, msg.as_string())\n",
    "        server.quit()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending email: {e}\")\n",
    "        return False\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"Resume Matcher with Advanced Retrieval\")\n",
    "\n",
    "# Input for job description\n",
    "job_description = st.text_area(\"Enter Job Description\")\n",
    "\n",
    "# Upload resumes\n",
    "uploaded_files = st.file_uploader(\"Upload Resumes\", accept_multiple_files=True, type=[\"pdf\", \"docx\", \"txt\"])\n",
    "\n",
    "# Input for number of top resumes to display\n",
    "top_n = st.number_input(\"Enter the number of top resumes to display\", min_value=1, value=5, step=1)\n",
    "\n",
    "# Dropdown for minimum education level\n",
    "education_levels = ['None', 'O level or equivalent', 'A level or equivalent', 'Diploma', 'Degree', 'Graduate Degree']\n",
    "min_education = st.selectbox(\"Select minimum education level required:\", education_levels, index=0)\n",
    "\n",
    "\n",
    "# Use session state to persist button clicks\n",
    "if 'matched_resumes' not in st.session_state:\n",
    "    st.session_state.matched_resumes = False\n",
    "    \n",
    "# Add a button to run the resume matching\n",
    "if st.button(\"Match Resumes\"):\n",
    "    st.session_state.matched_resumes = True\n",
    "    #st.success(\"Resumes have been matched.\")\n",
    "        \n",
    "# Only show the email option if resumes have been matched\n",
    "if st.session_state.matched_resumes:\n",
    "    \n",
    "    if job_description and uploaded_files:\n",
    "        resume_texts = []\n",
    "        filenames = []\n",
    "        resume_files = []  # To store the original file bytes\n",
    "\n",
    "        for uploaded_file in uploaded_files:\n",
    "            try:\n",
    "                file_content = extract_text(uploaded_file)\n",
    "                resume_texts.append(file_content)\n",
    "                filenames.append(uploaded_file.name)\n",
    "                resume_files.append(uploaded_file.getvalue())  # Store original file bytes\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error processing file {uploaded_file.name}: {e}\")\n",
    "\n",
    "        # Use the SemanticChunker to split resumes into meaningful chunks\n",
    "        embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "        text_splitter = SemanticChunker(embeddings_model)  # Semantic chunker instead of token-based\n",
    "\n",
    "        # Wrap text content into Document objects for LangChain processing\n",
    "        documents = [Document(page_content=text, metadata={\"filename\": filenames[i]}) for i, text in enumerate(resume_texts)]\n",
    "        semantic_chunks = []\n",
    "        for doc in documents:\n",
    "            chunks = text_splitter.split_documents([doc])\n",
    "            semantic_chunks.extend(chunks)\n",
    "\n",
    "        # Store chunks and embeddings in Chroma for retrieval\n",
    "        vectordb = Chroma.from_documents(semantic_chunks, embeddings_model)\n",
    "\n",
    "        # Create a LangChain RetrievalQA pipeline\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            retriever=vectordb.as_retriever(),\n",
    "            llm=llm\n",
    "        )\n",
    "\n",
    "        # Embed the job description for similarity scoring\n",
    "        job_desc_embedding = embeddings_model.embed_query(job_description)\n",
    "\n",
    "        # Generate summaries and calculate similarity scores\n",
    "        results = []\n",
    "        education_level_mapping = {\n",
    "            'O level or equivalent': 1,\n",
    "            'A level or equivalent': 2,\n",
    "            'Diploma': 3,\n",
    "            'Degree': 4,\n",
    "            'Graduate Degree': 5,\n",
    "            'Other': 0\n",
    "        }\n",
    "\n",
    "        # Map user's selected minimum education level\n",
    "        selected_min_education_level = education_level_mapping.get(min_education, 0)\n",
    "        \n",
    "        for idx, resume_text in enumerate(resume_texts):\n",
    "            # Get the embedding for the current resume\n",
    "            resume_embedding = embeddings_model.embed_query(resume_text)\n",
    "\n",
    "            # Calculate cosine similarity between job description and resume\n",
    "            similarity_score = cosine_similarity(\n",
    "                [job_desc_embedding], [resume_embedding]\n",
    "            )[0][0]\n",
    "\n",
    "            # Modify the prompt to request JSON-only output\n",
    "            result = qa_chain.run(f\"\"\"\n",
    "            For the following resume, extract and return the following details in JSON format only. Do not include any additional text or explanationsâ€”just the JSON data.\n",
    "\n",
    "            Required details:\n",
    "            - Name\n",
    "            - Education (list all tertiary degrees and above obtained, including degree name and field of study. If no tertiary degree then state the highest education achieved)\n",
    "            - Map the highest obtained education level to one of the following categories: \"O level or equivalent\", \"A level or equivalent\", \"Diploma\", \"Degree\", \"Graduate Degree\", \"Other\"\n",
    "            - Provide the mapped education level as \"Mapped Education Level\"\n",
    "            - Highlights of Career (2-3 standout sentences from the resume)\n",
    "\n",
    "            - Work experiences (list of job experiences), where each job experience includes:\n",
    "                - Job title\n",
    "                - Company name\n",
    "                - Start date\n",
    "                - End date\n",
    "\n",
    "            Example output:\n",
    "            {{\n",
    "                \"Name\": \"John Doe\",\n",
    "                \"Education\": [\n",
    "                    \"Bachelor in Computer Science\",\n",
    "                    \"Master of Business Administration\"\n",
    "                ],\n",
    "                \"Highlights of Career\": [\n",
    "                    \"Led a team of developers to create a cutting-edge AI application.\",\n",
    "                    \"Increased company revenue by 20% through innovative solutions.\"\n",
    "                ],\n",
    "                \"Work experiences\": [\n",
    "                    {{\"Job title\": \"Software Engineer\", \"Company name\": \"ABC Corp\", \"Start date\": \"Jan 2020\", \"End date\": \"Present\"}},\n",
    "                    {{\"Job title\": \"Junior Developer\", \"Company name\": \"XYZ Ltd\", \"Start date\": \"May 2017\", \"End date\": \"Dec 2019\"}}\n",
    "                ]\n",
    "            }}\n",
    "            Return only the JSON data. Do not include any additional text.\n",
    "\n",
    "            Resume: {resume_text[:1000]}...\n",
    "            \"\"\")\n",
    "\n",
    "            # Try parsing the result as JSON\n",
    "            try:\n",
    "                # Extract the JSON object from the result\n",
    "                json_match = re.search(r'\\{.*\\}', result, re.DOTALL)\n",
    "                if json_match:\n",
    "                    json_str = json_match.group()\n",
    "                    parsed_result = json.loads(json_str)\n",
    "                else:\n",
    "                    raise ValueError(\"No JSON object found in the model's output.\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Failed to parse result for {filenames[idx]}: {e}\")\n",
    "                st.write(\"Model's raw output:\")\n",
    "                st.write(result)\n",
    "                parsed_result = {}\n",
    "\n",
    "            name = parsed_result.get('Name', 'N/A')\n",
    "            education = parsed_result.get('Education', [])\n",
    "            mapped_education_level = parsed_result.get('Mapped Education Level', 'Other')\n",
    "            highlights = parsed_result.get('Highlights of Career', [])\n",
    "            job_experiences = parsed_result.get('Work experiences', [])\n",
    "            \n",
    "            # Map candidate's education level to numeric value\n",
    "            candidate_education_level = education_level_mapping.get(mapped_education_level, 0)\n",
    "\n",
    "\n",
    "            # Proceed with processing job_experiences as before\n",
    "            formatted_roles = []\n",
    "            for job in job_experiences:\n",
    "                job_title = job.get('Job title', 'N/A')\n",
    "                company = job.get('Company name', 'N/A')\n",
    "                start_date = job.get('Start date', 'N/A')\n",
    "                end_date = job.get('End date', 'N/A')\n",
    "                role_text = f\"{job_title} in {company} ({start_date} - {end_date})\"\n",
    "\n",
    "                # Calculate relevance score for this specific job role\n",
    "                role_embedding = embeddings_model.embed_query(role_text)\n",
    "                relevance_score = cosine_similarity(\n",
    "                    [job_desc_embedding], [role_embedding]\n",
    "                )[0][0] * 10  # Scale the relevance score (0-10 scale)\n",
    "\n",
    "                # Format the role with relevance score\n",
    "                formatted_roles.append(f\"{job_title} in {company} [Relevant score: {relevance_score:.1f}], {start_date} - {end_date}\")\n",
    "\n",
    "            # Append the summary, similarity score, and relevant roles to the results\n",
    "            results.append({\n",
    "                \"Filename\": filenames[idx],\n",
    "                \"Name\": name,\n",
    "                \"Education\": education,\n",
    "                \"Mapped Education Level\": mapped_education_level,\n",
    "                \"Candidate Education Level\": candidate_education_level,\n",
    "                \"Highlights\": highlights,\n",
    "                \"Relevant Years of Experience\": '\\n'.join(formatted_roles),\n",
    "                \"Similarity Score\": similarity_score,\n",
    "                \"Resume Text\": resume_text  # Include the resume text\n",
    "            })\n",
    "\n",
    "         # Filter resumes based on minimum education level\n",
    "        filtered_results = [result for result in results if result['Candidate Education Level'] >= selected_min_education_level]\n",
    "\n",
    "        # Check if any resumes meet the criteria\n",
    "        if not filtered_results:\n",
    "            st.warning(\"No resumes meet the minimum education level requirement.\")\n",
    "            st.stop()\n",
    "\n",
    "        # Sort filtered results by similarity score in descending order\n",
    "        filtered_results = sorted(filtered_results, key=lambda x: x[\"Similarity Score\"], reverse=True)\n",
    "\n",
    "        # Convert filtered results to DataFrame\n",
    "        df_results = pd.DataFrame(filtered_results)\n",
    "        \n",
    "        # Add a column to number each row starting from 1\n",
    "        df_results.insert(0, ' ', pd.Series(range(1, len(df_results) + 1)))\n",
    "\n",
    "        # For columns that are lists (Education, Highlights), join them into strings\n",
    "        df_results['Education'] = df_results['Education'].apply(lambda x: '; '.join(x) if isinstance(x, list) else x)\n",
    "        df_results['Highlights'] = df_results['Highlights'].apply(lambda x: '; '.join(x) if isinstance(x, list) else x)\n",
    "        df_results['Relevant Years of Experience'] = df_results['Relevant Years of Experience'].str.replace('\\n', '<br>')\n",
    "\n",
    "        # Reorder columns if necessary\n",
    "        df_results = df_results[[' ', 'Filename', 'Name', 'Education', 'Mapped Education Level', 'Highlights', 'Relevant Years of Experience', 'Similarity Score']]\n",
    "\n",
    "        # Create a brief ranking above the table\n",
    "        st.subheader(\"Resume Rankings\")\n",
    "        #for idx, row in df_results.iterrows():\n",
    "        #    if idx >= top_n:\n",
    "        #        break\n",
    "        #    with st.expander(f\"{idx + 1}. {row['Filename']} - Similarity Score: {row['Similarity Score']:.2f}\"):\n",
    "        #        st.write(results[idx]['Resume Text'])\n",
    "        \n",
    "        for idx, row in df_results.iterrows():\n",
    "            if idx >= top_n:\n",
    "                break\n",
    "\n",
    "            # Use the ' ' column for the counter/numbering\n",
    "            counter = row[' ']  # Refers to the numbering column\n",
    "\n",
    "            # Create two columns: one for numbering and one for the expander\n",
    "            col1, col2 = st.columns([1, 9])  # Adjust the ratio as needed for layout\n",
    "\n",
    "            # Display the counter in the first column\n",
    "            with col1:\n",
    "                st.write(f\"{counter}\")\n",
    "\n",
    "            # Display the expander in the second column\n",
    "            with col2:\n",
    "                with st.expander(f\"{row['Filename']} - Similarity Score: {row['Similarity Score']:.2f}\"):\n",
    "                    st.write(results[idx]['Resume Text'])\n",
    "\n",
    "        # Display the DataFrame without modifying the 'Filename' column\n",
    "        st.write(\"Detailed Resume Summaries\")\n",
    "        #st.dataframe(df_results)\n",
    "        st.write(df_results.to_html(escape=False, index=False), unsafe_allow_html=True)\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "        # Input field for hiring manager's email\n",
    "        recipient_email = st.text_input(\"Enter hiring manager's email:\")\n",
    "\n",
    "        # Button to send the summary table\n",
    "        if st.button(\"Forward Summary to Hiring Manager\"):\n",
    "            if recipient_email:\n",
    "                # Convert DataFrame to CSV\n",
    "                # Replace <br> with newline characters '\\n' before saving to CSV\n",
    "                df_results['Relevant Years of Experience'] = df_results['Relevant Years of Experience'].str.replace('<br>', '\\n')\n",
    "    \n",
    "                csv_data = df_results.to_csv(index=False)\n",
    "\n",
    "                # Prepare the email content\n",
    "                subject = \"Resume Summaries\"\n",
    "                message = \"Dear Hiring Manager,\\n\\nPlease find attached the summaries of the top resumes.\\n\\nBest regards,\\nYour Name\"\n",
    "\n",
    "                # Send the email with attachment\n",
    "                success = send_email(\n",
    "                    sender_email, sender_password, recipient_email,\n",
    "                    subject, message,\n",
    "                    attachment=csv_data.encode('utf-8-sig'),\n",
    "                    attachment_filename='resume_summaries.csv'\n",
    "                )\n",
    "                if success:\n",
    "                    st.success(f\"Email sent to {recipient_email}\")\n",
    "                else:\n",
    "                    st.error(f\"Failed to send email to {recipient_email}\")\n",
    "            else:\n",
    "                st.error(\"Please enter a recipient email address.\")\n",
    "                \n",
    "                        \n",
    "    else:\n",
    "        st.error(\"Please enter a job description and upload at least one resume.\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6a3d7a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run resume_screen_testapp2.py --server.enableXsrfProtection false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3603d7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting resume_screen_testapp2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile resume_screen_testapp2.py\n",
    "##########################################FINAL VERSION ######################################################\n",
    "\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PyPDF2 import PdfReader\n",
    "import docx\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Set the page layout to wide\n",
    "st.set_page_config(layout=\"wide\")\n",
    "\n",
    "def add_bg_from_url():\n",
    "    st.markdown(\n",
    "         f\"\"\"\n",
    "         <style>\n",
    "         .stApp {{\n",
    "             #background-image: url(\"https://cdn.pixabay.com/photo/2016/10/29/10/12/purple-1780371_1280.png\");\n",
    "             background-image: linear-gradient(rgba(0, 0, 0, 0.5), rgba(255, 255, 255, 0.75)), url(\"https://cdn.pixabay.com/photo/2016/10/29/10/12/purple-1780371_1280.png\");\n",
    "             background-attachment: fixed;\n",
    "             background-size: cover\n",
    "         }}\n",
    "         /* Ensure the font color stays black */\n",
    "         .stApp * {{\n",
    "             color: black !important;\n",
    "         }}\n",
    "         </style>\n",
    "         \"\"\",\n",
    "         unsafe_allow_html=True\n",
    "     )\n",
    "\n",
    "add_bg_from_url() \n",
    "\n",
    "\n",
    "# Path to your header image\n",
    "header_image_path = \"C:/Users/weichee/Documents/ai bootcamp/headerc.png\"\n",
    "header_image = Image.open(header_image_path)\n",
    "\n",
    "# Display the header image\n",
    "col1 = st.columns(1)[0]\n",
    "with col1:\n",
    "    st.image(header_image, caption=\"\", use_column_width=True)\n",
    "\n",
    "    \n",
    "# Load button images using PIL\n",
    "button_image_path1 = \"C:/Users/weichee/Documents/ai bootcamp/data extraction button 1.png\"\n",
    "button_image_path2 = \"C:/Users/weichee/Documents/ai bootcamp/data extraction button 2.png\"\n",
    "\n",
    "button_image1 = Image.open(button_image_path1)\n",
    "button_image2 = Image.open(button_image_path2)\n",
    "\n",
    "# Section title\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "    <div style='text-align: center; font-size: 40px; font-weight: bold; margin-top: 50px; margin-bottom: 50px;'>\n",
    "        Choose Your Resume Extraction Method\n",
    "    </div>\n",
    "    \"\"\", \n",
    "    unsafe_allow_html=True\n",
    ")\n",
    "###############################################################################\n",
    "# Create columns for the buttons\n",
    "##col1, col2, col3, col4 = st.columns(4)\n",
    "\n",
    "# Display the images with a \"Get Started\" button below each\n",
    "##with col2:\n",
    "##    st.image(button_image1, caption=\"\", use_column_width=True)\n",
    "##    if st.button(\"Get Started\", key=\"simulated\"):\n",
    "##        st.session_state[\"button_clicked\"] = \"Simulated Website Extraction\"\n",
    "\n",
    "##with col3:\n",
    "##    st.image(button_image2, caption=\"\", use_column_width=True)\n",
    "##    if st.button(\"Get Started\", key=\"manual\"):\n",
    "##        st.session_state[\"button_clicked\"] = \"Manual Upload\"\n",
    "        \n",
    "############################################################################\n",
    "        \n",
    "# Create columns with custom widths\n",
    "col1, col2, col3, col4, col5 = st.columns([2, 2, 1, 2, 2])\n",
    "\n",
    "# Display the first image and button in col2\n",
    "with col2:\n",
    "    st.image(button_image1, caption=\"\", use_column_width=True)\n",
    "    if st.button(\"Get Started\", key=\"simulated\"):\n",
    "        st.session_state[\"button_clicked\"] = \"Simulated Website Extraction\"\n",
    "\n",
    "# Display the second image and button in col4\n",
    "with col4:\n",
    "    st.image(button_image2, caption=\"\", use_column_width=True)\n",
    "    if st.button(\"Get Started\", key=\"manual\"):\n",
    "        st.session_state[\"button_clicked\"] = \"Manual Upload\"\n",
    "################################################################################\n",
    "\n",
    "# Function to read PDF files\n",
    "def read_pdf(file):\n",
    "    text = \"\"\n",
    "    reader = PdfReader(file)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to read Word documents\n",
    "def read_word(file):\n",
    "    doc = docx.Document(file)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# Function to read text files\n",
    "def read_txt(file):\n",
    "    try:\n",
    "        return file.read().decode(\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        return file.read().decode(\"ISO-8859-1\")\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def compute_similarity(job_description, resumes):\n",
    "    documents = [job_description] + resumes\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "    return cosine_sim\n",
    "\n",
    "# Load the pre-trained Sentence-BERT model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')  # A small and fast model for this use case\n",
    "\n",
    "# Function to compute similarity using Sentence-BERT\n",
    "def compute_similarity(job_description, resumes):\n",
    "    # Encode the job description and all resumes into embeddings\n",
    "    job_desc_embedding = model.encode(job_description, convert_to_tensor=True)\n",
    "    resume_embeddings = model.encode(resumes, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarity between job description and each resume\n",
    "    cosine_sim = util.cos_sim(job_desc_embedding, resume_embeddings)\n",
    "\n",
    "    # Convert cosine similarities from tensors to a list of scores\n",
    "    similarity_scores = cosine_sim.squeeze().tolist()\n",
    "    \n",
    "    return similarity_scores\n",
    "\n",
    "# Check which button was clicked and display the corresponding content\n",
    "if st.session_state.get(\"button_clicked\") == \"Simulated Website Extraction\":\n",
    "    st.header(\"Simulated Website Extraction\")\n",
    "    st.write(\"This option simulates resume and job description extraction as if from a website.\")\n",
    "    \n",
    "    # Input for job title (smaller text input)\n",
    "    job_title = st.text_input(\"Enter Job Title\", max_chars=50)\n",
    "\n",
    "    # Input for date range\n",
    "    start_date = st.date_input(\"Start Date (Submission)\")\n",
    "    end_date = st.date_input(\"End Date (Submission)\")\n",
    "\n",
    "    # Input for number of top resumes to display\n",
    "    top_n = st.number_input(\"Enter number of top resumes to display\", min_value=1, value=3)\n",
    "\n",
    "    # Function to filter resumes based on the date range\n",
    "    def filter_resumes_by_date(resume_folder, start_date, end_date):\n",
    "        filtered_resumes = []\n",
    "        for file_name in os.listdir(resume_folder):\n",
    "            file_path = os.path.join(resume_folder, file_name)\n",
    "            if not file_name.endswith(\".txt\"):  # Assuming all resumes are non-txt files\n",
    "                modification_time = os.path.getmtime(file_path)\n",
    "                modification_date = datetime.fromtimestamp(modification_time).date()\n",
    "                if start_date <= modification_date <= end_date:\n",
    "                    filtered_resumes.append(file_name)\n",
    "        return filtered_resumes\n",
    "    \n",
    "        # Function to read different resume formats\n",
    "    def read_resume(file_path):\n",
    "        if file_path.endswith(\".pdf\"):\n",
    "            return read_pdf(file_path)\n",
    "        elif file_path.endswith(\".doc\") or file_path.endswith(\".docx\"):\n",
    "            return read_word(file_path)\n",
    "        elif file_path.endswith(\".txt\"):\n",
    "            return read_txt(file_path)\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    # Button to trigger resume matching\n",
    "    if st.button(\"Match Resumes\"):\n",
    "        if job_title:\n",
    "            # Define the path based on the job title\n",
    "            folder_path = os.path.join(\"C:\\\\Users\\\\weichee\\\\Documents\\\\ai bootcamp\", job_title)\n",
    "\n",
    "            # Check if the folder exists\n",
    "            if os.path.exists(folder_path):\n",
    "                # Extract the job description\n",
    "                job_description_file = os.path.join(folder_path, f\"{job_title}.txt\")\n",
    "                if os.path.exists(job_description_file):\n",
    "                    with open(job_description_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                        job_description = file.read()\n",
    "\n",
    "                    # Filter resumes by date\n",
    "                    filtered_resumes = filter_resumes_by_date(folder_path, start_date, end_date)\n",
    "                    if filtered_resumes:\n",
    "                        # Read the content of each resume\n",
    "                        resumes_content = []\n",
    "                        for resume in filtered_resumes:\n",
    "                            resume_content = read_resume(os.path.join(folder_path, resume))\n",
    "                            resumes_content.append(resume_content)\n",
    "\n",
    "                        # Compute cosine similarity\n",
    "                        similarity_scores = compute_similarity(job_description, resumes_content)\n",
    "\n",
    "                        # Combine the resumes with their similarity scores\n",
    "                        resume_scores = list(zip(filtered_resumes, similarity_scores))\n",
    "\n",
    "                        # Sort resumes by similarity score in descending order\n",
    "                        sorted_resumes = sorted(resume_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                        # Display the top N resumes based on the user's input\n",
    "                        st.subheader(f\"Top {top_n} Resumes\")\n",
    "                        for i, (resume, score) in enumerate(sorted_resumes[:top_n]):\n",
    "                            st.write(f\"{i + 1}. {resume} - Similarity Score: {score:.4f}\")\n",
    "                    else:\n",
    "                        st.warning(\"No resumes found within the specified date range.\")\n",
    "                else:\n",
    "                    st.error(\"Job description file not found.\")\n",
    "            else:\n",
    "                st.error(\"Job title folder not found.\")\n",
    "        else:\n",
    "            st.error(\"Please enter a job title.\")\n",
    "\n",
    "elif st.session_state.get(\"button_clicked\") == \"Manual Upload\":\n",
    "    st.header(\"Manual Upload\")\n",
    "    uploaded_resumes = st.file_uploader(\"Upload Resumes\", accept_multiple_files=True)\n",
    "    job_description = st.text_area(\"Enter Job Description\")\n",
    "\n",
    "    # Button to trigger resume matching, visible from the start\n",
    "    if st.button(\"Match Resumes\"):\n",
    "        if uploaded_resumes and job_description:\n",
    "            resumes_content = []\n",
    "            for uploaded_file in uploaded_resumes:\n",
    "                if uploaded_file.type == \"application/pdf\":\n",
    "                    resumes_content.append(read_pdf(uploaded_file))\n",
    "                elif uploaded_file.type in [\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\", \"application/msword\"]:\n",
    "                    resumes_content.append(read_word(uploaded_file))\n",
    "                elif uploaded_file.type == \"text/plain\":\n",
    "                    resumes_content.append(uploaded_file.read().decode(\"utf-8\"))\n",
    "\n",
    "            # Compute cosine similarity\n",
    "            similarity_scores = compute_similarity(job_description, resumes_content)\n",
    "\n",
    "            # Combine the resumes with their similarity scores\n",
    "            resume_scores = list(zip([uploaded_file.name for uploaded_file in uploaded_resumes], similarity_scores))\n",
    "\n",
    "            # Sort resumes by similarity score in descending order\n",
    "            sorted_resumes = sorted(resume_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # Display the top N resumes based on the user's input\n",
    "            st.subheader(\"Ranked Resumes\")\n",
    "            for i, (resume, score) in enumerate(sorted_resumes):\n",
    "                st.write(f\"{i + 1}. {resume} - Similarity Score: {score:.4f}\")\n",
    "        else:\n",
    "            st.warning(\"Please upload resumes and provide a job description.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a0b8a24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run resume_screen_testapp2.py --server.enableXsrfProtection false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "966f448f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4c0386",
   "metadata": {},
   "outputs": [],
   "source": [
    "For each of the following resumes, extract the following details:\n",
    "- Name\n",
    "- Area of study\n",
    "- Years of experience\n",
    "- Key skills\n",
    "\n",
    "Resume 1: [resume 1 text]\n",
    "Resume 2: [resume 2 text]\n",
    "Resume 3: [resume 3 text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8a7280c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting resume_screen_testapp2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile resume_screen_testapp2.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "import PyPDF2\n",
    "import json\n",
    "import re\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.docstore.document import Document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.application import MIMEApplication\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv('.env')\n",
    "\n",
    "# Fetch the OpenAI API key and email credentials from the environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "sender_email = os.getenv('SENDER_EMAIL')\n",
    "sender_password = os.getenv('SENDER_PASSWORD')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Function to extract text from PDF files\n",
    "def extract_text_from_pdf(file):\n",
    "    text = \"\"\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to extract text from DOCX files\n",
    "def extract_text_from_docx(file):\n",
    "    return docx2txt.process(file)\n",
    "\n",
    "# Function to extract text from TXT files\n",
    "def extract_text_from_txt(file):\n",
    "    return file.read().decode(\"utf-8\")\n",
    "\n",
    "# Function to extract text from different file types\n",
    "def extract_text(uploaded_file):\n",
    "    if uploaded_file.type == \"application/pdf\":\n",
    "        return extract_text_from_pdf(uploaded_file)\n",
    "    elif uploaded_file.type == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\":\n",
    "        return extract_text_from_docx(uploaded_file)\n",
    "    elif uploaded_file.type == \"text/plain\":\n",
    "        return extract_text_from_txt(uploaded_file)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Function to send email with attachment\n",
    "def send_email(sender_email, sender_password, recipient_email, subject, message, attachment=None, attachment_filename='attachment.csv'):\n",
    "    try:\n",
    "        # Create the email message\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = sender_email\n",
    "        msg['To'] = recipient_email\n",
    "        msg['Subject'] = subject\n",
    "\n",
    "        # Add message body\n",
    "        msg.attach(MIMEText(message, 'plain'))\n",
    "\n",
    "        # Attach the CSV file if provided\n",
    "        if attachment is not None:\n",
    "            part = MIMEApplication(attachment, Name=attachment_filename)\n",
    "            part['Content-Disposition'] = f'attachment; filename=\"{attachment_filename}\"'\n",
    "            msg.attach(part)\n",
    "\n",
    "        # Connect to SMTP server using STARTTLS\n",
    "        server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "        server.starttls()\n",
    "        server.login(sender_email, sender_password)\n",
    "\n",
    "        # Send the email\n",
    "        server.sendmail(sender_email, recipient_email, msg.as_string())\n",
    "        server.quit()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error sending email: {e}\")\n",
    "        return False\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"Resume Matcher with Advanced Retrieval\")\n",
    "\n",
    "# Input for job description\n",
    "job_description = st.text_area(\"Enter Job Description\")\n",
    "\n",
    "# Upload resumes\n",
    "uploaded_files = st.file_uploader(\"Upload Resumes\", accept_multiple_files=True, type=[\"pdf\", \"docx\", \"txt\"])\n",
    "\n",
    "# Input for number of top resumes to display\n",
    "top_n = st.number_input(\"Enter the number of top resumes to display\", min_value=1, value=5, step=1)\n",
    "\n",
    "# Dropdown for minimum education level\n",
    "education_levels = ['None', 'O level or equivalent', 'A level or equivalent', 'Diploma', 'Degree', 'Graduate Degree']\n",
    "min_education = st.selectbox(\"Select minimum education level required:\", education_levels, index=0)\n",
    "\n",
    "# Initialize session state variables\n",
    "if 'matched_resumes' not in st.session_state:\n",
    "    st.session_state.matched_resumes = False\n",
    "if 'results' not in st.session_state:\n",
    "    st.session_state.results = None\n",
    "\n",
    "# Function to process resumes\n",
    "def process_resumes(job_description, uploaded_files):\n",
    "    resume_texts = []\n",
    "    filenames = []\n",
    "    resume_files = []  # To store the original file bytes\n",
    "\n",
    "    for uploaded_file in uploaded_files:\n",
    "        try:\n",
    "            file_content = extract_text(uploaded_file)\n",
    "            resume_texts.append(file_content)\n",
    "            filenames.append(uploaded_file.name)\n",
    "            resume_files.append(uploaded_file.getvalue())  # Store original file bytes\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error processing file {uploaded_file.name}: {e}\")\n",
    "\n",
    "    # Use the SemanticChunker to split resumes into meaningful chunks\n",
    "    embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    text_splitter = SemanticChunker(embeddings_model)  # Semantic chunker instead of token-based\n",
    "\n",
    "    # Wrap text content into Document objects for LangChain processing\n",
    "    documents = [Document(page_content=text, metadata={\"filename\": filenames[i]}) for i, text in enumerate(resume_texts)]\n",
    "    semantic_chunks = []\n",
    "    for doc in documents:\n",
    "        chunks = text_splitter.split_documents([doc])\n",
    "        semantic_chunks.extend(chunks)\n",
    "\n",
    "    # Store chunks and embeddings in Chroma for retrieval\n",
    "    vectordb = Chroma.from_documents(semantic_chunks, embeddings_model)\n",
    "\n",
    "    # Create a LangChain RetrievalQA pipeline\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        retriever=vectordb.as_retriever(),\n",
    "        llm=llm\n",
    "    )\n",
    "\n",
    "    # Embed the job description for similarity scoring\n",
    "    job_desc_embedding = embeddings_model.embed_query(job_description)\n",
    "\n",
    "    # Generate summaries and calculate similarity scores\n",
    "    results = []\n",
    "    education_level_mapping = {\n",
    "        'O level or equivalent': 1,\n",
    "        'A level or equivalent': 2,\n",
    "        'Diploma': 3,\n",
    "        'Degree': 4,\n",
    "        'Graduate Degree': 5,\n",
    "        'Other': 0\n",
    "    }\n",
    "\n",
    "    for idx, resume_text in enumerate(resume_texts):\n",
    "        # Get the embedding for the current resume\n",
    "        resume_embedding = embeddings_model.embed_query(resume_text)\n",
    "\n",
    "        # Calculate cosine similarity between job description and resume\n",
    "        similarity_score = cosine_similarity(\n",
    "            [job_desc_embedding], [resume_embedding]\n",
    "        )[0][0]\n",
    "\n",
    "        # Modify the prompt to request JSON-only output\n",
    "        result = qa_chain.run(f\"\"\"\n",
    "        For the following resume, extract and return the following details in JSON format only. Do not include any additional text or explanationsâ€”just the JSON data.\n",
    "\n",
    "        Required details:\n",
    "        - Name\n",
    "        - Education (list all tertiary degrees and above obtained, including degree name and field of study. If no tertiary degree then state the highest education achieved)\n",
    "        - Map the highest obtained education level to one of the following categories: \"O level or equivalent\", \"A level or equivalent\", \"Diploma\", \"Degree\", \"Graduate Degree\", \"Other\"\n",
    "        - Provide the mapped education level as \"Mapped Education Level\"\n",
    "        - Highlights of Career (2-3 standout sentences from the resume)\n",
    "\n",
    "        - Work experiences (list of job experiences), where each job experience includes:\n",
    "            - Job title\n",
    "            - Company name\n",
    "            - Start date\n",
    "            - End date\n",
    "\n",
    "        Example output:\n",
    "        {{\n",
    "            \"Name\": \"John Doe\",\n",
    "            \"Education\": [\n",
    "                \"Bachelor in Computer Science\",\n",
    "                \"Master of Business Administration\"\n",
    "            ],\n",
    "            \"Mapped Education Level\": \"Graduate Degree\",\n",
    "            \"Highlights of Career\": [\n",
    "                \"Led a team of developers to create a cutting-edge AI application.\",\n",
    "                \"Increased company revenue by 20% through innovative solutions.\"\n",
    "            ],\n",
    "            \"Work experiences\": [\n",
    "                {{\"Job title\": \"Software Engineer\", \"Company name\": \"ABC Corp\", \"Start date\": \"Jan 2020\", \"End date\": \"Present\"}},\n",
    "                {{\"Job title\": \"Junior Developer\", \"Company name\": \"XYZ Ltd\", \"Start date\": \"May 2017\", \"End date\": \"Dec 2019\"}}\n",
    "            ]\n",
    "        }}\n",
    "        Return only the JSON data. Do not include any additional text.\n",
    "\n",
    "        Resume: {resume_text[:1000]}...\n",
    "        \"\"\")\n",
    "\n",
    "        # Try parsing the result as JSON\n",
    "        try:\n",
    "            # Extract the JSON object from the result\n",
    "            json_match = re.search(r'\\{.*\\}', result, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group()\n",
    "                parsed_result = json.loads(json_str)\n",
    "            else:\n",
    "                raise ValueError(\"No JSON object found in the model's output.\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"Failed to parse result for {filenames[idx]}: {e}\")\n",
    "            st.write(\"Model's raw output:\")\n",
    "            st.write(result)\n",
    "            parsed_result = {}\n",
    "\n",
    "        name = parsed_result.get('Name', 'N/A')\n",
    "        education = parsed_result.get('Education', [])\n",
    "        mapped_education_level = parsed_result.get('Mapped Education Level', 'Other')\n",
    "        highlights = parsed_result.get('Highlights of Career', [])\n",
    "        job_experiences = parsed_result.get('Work experiences', [])\n",
    "\n",
    "        # Map candidate's education level to numeric value\n",
    "        candidate_education_level = education_level_mapping.get(mapped_education_level, 0)\n",
    "\n",
    "        # Proceed with processing job_experiences as before\n",
    "        formatted_roles = []\n",
    "        for job in job_experiences:\n",
    "            job_title = job.get('Job title', 'N/A')\n",
    "            company = job.get('Company name', 'N/A')\n",
    "            start_date = job.get('Start date', 'N/A')\n",
    "            end_date = job.get('End date', 'N/A')\n",
    "            role_text = f\"{job_title} in {company} ({start_date} - {end_date})\"\n",
    "\n",
    "            # Calculate relevance score for this specific job role\n",
    "            role_embedding = embeddings_model.embed_query(role_text)\n",
    "            relevance_score = cosine_similarity(\n",
    "                [job_desc_embedding], [role_embedding]\n",
    "            )[0][0] * 10  # Scale the relevance score (0-10 scale)\n",
    "\n",
    "            # Format the role with relevance score\n",
    "            formatted_roles.append(f\"{job_title} in {company} [Relevant score: {relevance_score:.1f}], {start_date} - {end_date}\")\n",
    "\n",
    "        # Append the summary, similarity score, and relevant roles to the results\n",
    "        results.append({\n",
    "            \"Filename\": filenames[idx],\n",
    "            \"Name\": name,\n",
    "            \"Education\": education,\n",
    "            \"Mapped Education Level\": mapped_education_level,\n",
    "            \"Candidate Education Level\": candidate_education_level,\n",
    "            \"Highlights\": highlights,\n",
    "            \"Relevant Years of Experience\": '\\n'.join(formatted_roles),\n",
    "            \"Similarity Score\": similarity_score,\n",
    "            \"Resume Text\": resume_text  # Include the resume text\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Add a button to run the resume matching\n",
    "if st.button(\"Match Resumes\"):\n",
    "    if job_description and uploaded_files:\n",
    "        st.session_state.matched_resumes = True\n",
    "        # Process resumes and store results\n",
    "        results = process_resumes(job_description, uploaded_files)\n",
    "        st.session_state.results = results\n",
    "    else:\n",
    "        st.error(\"Please enter a job description and upload at least one resume.\")\n",
    "\n",
    "# Only proceed if resumes have been matched and results are available\n",
    "if st.session_state.matched_resumes and st.session_state.results is not None:\n",
    "    results = st.session_state.results\n",
    "\n",
    "    # Map education levels to numeric values\n",
    "    education_level_mapping = {\n",
    "        'O level or equivalent': 1,\n",
    "        'A level or equivalent': 2,\n",
    "        'Diploma': 3,\n",
    "        'Degree': 4,\n",
    "        'Graduate Degree': 5,\n",
    "        'Other': 6\n",
    "    }\n",
    "\n",
    "    # Map user's selected minimum education level\n",
    "    selected_min_education_level = education_level_mapping.get(min_education, 0)\n",
    "\n",
    "    # Filter resumes based on minimum education level\n",
    "    filtered_results = [result for result in results if result['Candidate Education Level'] >= selected_min_education_level]\n",
    "\n",
    "    # Check if any resumes meet the criteria\n",
    "    if not filtered_results:\n",
    "        st.warning(\"No resumes meet the minimum education level requirement.\")\n",
    "    else:\n",
    "        # Sort filtered results by similarity score in descending order\n",
    "        filtered_results = sorted(filtered_results, key=lambda x: x[\"Similarity Score\"], reverse=True)\n",
    "\n",
    "        # Convert filtered results to DataFrame\n",
    "        df_results = pd.DataFrame(filtered_results)\n",
    "\n",
    "        # Add a column to number each row starting from 1\n",
    "        df_results.insert(0, ' ', pd.Series(range(1, len(df_results) + 1)))\n",
    "\n",
    "        # For columns that are lists (Education, Highlights), join them into strings\n",
    "        df_results['Education'] = df_results['Education'].apply(lambda x: '; '.join(x) if isinstance(x, list) else x)\n",
    "        df_results['Highlights'] = df_results['Highlights'].apply(lambda x: '; '.join(x) if isinstance(x, list) else x)\n",
    "        # No need to replace '\\n' with '<br>' here since we're using st.write\n",
    "        # df_results['Relevant Years of Experience'] = df_results['Relevant Years of Experience'].str.replace('\\n', '<br>')\n",
    "\n",
    "        # Reorder columns if necessary\n",
    "        df_results = df_results[[' ', 'Filename', 'Name', 'Education', 'Mapped Education Level', 'Highlights', 'Relevant Years of Experience', 'Similarity Score', 'Resume Text']]\n",
    "\n",
    "        # Create a brief ranking above the table\n",
    "        st.subheader(\"Resume Rankings\")\n",
    "        for idx, row in df_results.iterrows():\n",
    "            if idx >= top_n:\n",
    "                break\n",
    "\n",
    "            # Use the ' ' column for the counter/numbering\n",
    "            counter = row[' ']  # Refers to the numbering column\n",
    "\n",
    "            # Create two columns: one for numbering and one for the expander\n",
    "            col1, col2 = st.columns([1, 9])  # Adjust the ratio as needed for layout\n",
    "\n",
    "            # Display the counter in the first column\n",
    "            with col1:\n",
    "                st.write(f\"{counter}\")\n",
    "\n",
    "            # Display the expander in the second column\n",
    "            with col2:\n",
    "                with st.expander(f\"{row['Filename']} - Similarity Score: {row['Similarity Score']:.2f}\"):\n",
    "                    st.write(row['Resume Text'])\n",
    "\n",
    "        # Display the DataFrame without modifying the 'Filename' column\n",
    "        st.write(\"Detailed Resume Summaries\")\n",
    "        # Exclude 'Resume Text' from the displayed DataFrame\n",
    "        df_display = df_results.drop(columns=['Resume Text'])\n",
    "        st.write(df_display.to_html(escape=False, index=False), unsafe_allow_html=True)\n",
    "\n",
    "        # Input field for hiring manager's email\n",
    "        recipient_email = st.text_input(\"Enter hiring manager's email:\")\n",
    "\n",
    "        # Button to send the summary table\n",
    "        if st.button(\"Forward Summary to Hiring Manager\"):\n",
    "            if recipient_email:\n",
    "                # Prepare DataFrame for CSV\n",
    "                # Replace '<br>' with newline characters '\\n' if necessary\n",
    "                df_results['Relevant Years of Experience'] = df_results['Relevant Years of Experience'].str.replace('<br>', '\\n')\n",
    "\n",
    "                # Exclude 'Resume Text' from the CSV\n",
    "                df_csv = df_results.drop(columns=['Resume Text'])\n",
    "                csv_data = df_csv.to_csv(index=False)\n",
    "\n",
    "                # Prepare the email content\n",
    "                subject = \"Resume Summaries\"\n",
    "                message = \"Dear Hiring Manager,\\n\\nPlease find attached the summaries of the top resumes.\\n\\nBest regards,\\nYour Name\"\n",
    "\n",
    "                # Send the email with attachment\n",
    "                success = send_email(\n",
    "                    sender_email, sender_password, recipient_email,\n",
    "                    subject, message,\n",
    "                    attachment=csv_data.encode('utf-8-sig'),\n",
    "                    attachment_filename='resume_summaries.csv'\n",
    "                )\n",
    "                if success:\n",
    "                    st.success(f\"Email sent to {recipient_email}\")\n",
    "                else:\n",
    "                    st.error(f\"Failed to send email to {recipient_email}\")\n",
    "            else:\n",
    "                st.error(\"Please enter a recipient email address.\")\n",
    "else:\n",
    "    if st.session_state.matched_resumes:\n",
    "        st.info(\"Processing resumes. Please wait...\")\n",
    "    else:\n",
    "        st.info(\"Please click 'Match Resumes' to process the resumes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "90478a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#it works\n",
    "!streamlit run resume_screen_testapp2.py --server.enableXsrfProtection false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "78603241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting resume_screen_testapp2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile resume_screen_testapp2.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "import PyPDF2\n",
    "import json\n",
    "import re\n",
    "import openai\n",
    "import os\n",
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.docstore.document import Document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.application import MIMEApplication\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv('.env')\n",
    "\n",
    "# Fetch the OpenAI API key and email credentials from the environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "sender_email = os.getenv('SENDER_EMAIL')\n",
    "sender_password = os.getenv('SENDER_PASSWORD')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Function to extract text from PDF files\n",
    "def extract_text_from_pdf(file):\n",
    "    text = \"\"\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to extract text from DOCX files\n",
    "def extract_text_from_docx(file):\n",
    "    return docx2txt.process(file)\n",
    "\n",
    "# Function to extract text from TXT files\n",
    "def extract_text_from_txt(file):\n",
    "    return file.read().decode(\"utf-8\")\n",
    "\n",
    "# Function to extract text from different file types\n",
    "def extract_text(uploaded_file):\n",
    "    if uploaded_file.name.lower().endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(uploaded_file)\n",
    "    elif uploaded_file.name.lower().endswith(\".docx\"):\n",
    "        return extract_text_from_docx(uploaded_file)\n",
    "    elif uploaded_file.name.lower().endswith(\".txt\"):\n",
    "        return extract_text_from_txt(uploaded_file)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Function to send email with attachment\n",
    "def send_email(sender_email, sender_password, recipient_email, subject, message, attachment=None, attachment_filename='attachment.csv'):\n",
    "    try:\n",
    "        # Create the email message\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = sender_email\n",
    "        msg['To'] = recipient_email\n",
    "        msg['Subject'] = subject\n",
    "\n",
    "        # Add message body\n",
    "        msg.attach(MIMEText(message, 'plain'))\n",
    "\n",
    "        # Attach the CSV file if provided\n",
    "        if attachment is not None:\n",
    "            part = MIMEApplication(attachment, Name=attachment_filename)\n",
    "            part['Content-Disposition'] = f'attachment; filename=\"{attachment_filename}\"'\n",
    "            msg.attach(part)\n",
    "\n",
    "        # Connect to SMTP server using STARTTLS\n",
    "        server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "        server.starttls()\n",
    "        server.login(sender_email, sender_password)\n",
    "\n",
    "        # Send the email\n",
    "        server.sendmail(sender_email, recipient_email, msg.as_string())\n",
    "        server.quit()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error sending email: {e}\")\n",
    "        return False\n",
    "# Set the page layout to wide\n",
    "st.set_page_config(layout=\"wide\")\n",
    "\n",
    "\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"Resume Matcher with Advanced Retrieval\")\n",
    "\n",
    "# Selection for resume source\n",
    "resume_source = st.radio(\"Select Resume Source\", ('Automated Resume Extraction', 'Manual Upload'))\n",
    "\n",
    "# Initialize session state variables\n",
    "if 'matched_resumes' not in st.session_state:\n",
    "    st.session_state.matched_resumes = False\n",
    "if 'results' not in st.session_state:\n",
    "    st.session_state.results = None\n",
    "\n",
    "# Sample resumes with submission dates (simulate automated extraction)\n",
    "sample_resumes = [\n",
    "    {'filename': 'resume1_KENNY SIM.docx', 'submission_date': datetime.date(2023, 9, 1)},\n",
    "    {'filename': 'resume2_crystal.docx', 'submission_date': datetime.date(2023, 9, 5)},\n",
    "    {'filename': 'resume3_mark.docx', 'submission_date': datetime.date(2023, 9, 10)},\n",
    "    {'filename': 'resume4_david.docx', 'submission_date': datetime.date(2023, 9, 15)},\n",
    "    {'filename': 'resume5_linus_ds.docx', 'submission_date': datetime.date(2023, 10, 15)}\n",
    "]\n",
    "\n",
    "# Depending on resume source, display appropriate inputs\n",
    "if resume_source == 'Manual Upload':\n",
    "    # Input for job description\n",
    "    job_description = st.text_area(\"Enter Job Description\")\n",
    "    # Upload resumes\n",
    "    uploaded_files = st.file_uploader(\"Upload Resumes\", accept_multiple_files=True, type=[\"pdf\", \"docx\", \"txt\"])\n",
    "elif resume_source == 'Automated Resume Extraction':\n",
    "    # Input for job title/job code\n",
    "    job_title = st.text_input(\"Enter Job Title/Code\")\n",
    "    # Date range picker for resume submission date\n",
    "    date_range = st.date_input(\"Select Resume Submission Date Range\", value=(datetime.date(2023, 9, 1), datetime.date(2023, 9, 30)))\n",
    "    if isinstance(date_range, tuple):\n",
    "        start_date, end_date = date_range\n",
    "    else:\n",
    "        start_date = end_date = date_range\n",
    "\n",
    "# Input for number of top resumes to display\n",
    "top_n = st.number_input(\"Enter the number of top resumes to display\", min_value=1, value=5, step=1)\n",
    "\n",
    "# Dropdown for minimum education level\n",
    "education_levels = ['None', 'O level or equivalent', 'A level or equivalent', 'Diploma', 'Degree', 'Graduate Degree']\n",
    "min_education = st.selectbox(\"Select minimum education level required:\", education_levels, index=0)\n",
    "\n",
    "# Function to process resumes\n",
    "def process_resumes(job_description, resume_texts, filenames):\n",
    "    # Use the SemanticChunker to split resumes into meaningful chunks\n",
    "    embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    text_splitter = SemanticChunker(embeddings_model)  # Semantic chunker instead of token-based\n",
    "\n",
    "    # Wrap text content into Document objects for LangChain processing\n",
    "    documents = [Document(page_content=text, metadata={\"filename\": filenames[i]}) for i, text in enumerate(resume_texts)]\n",
    "    semantic_chunks = []\n",
    "    for doc in documents:\n",
    "        chunks = text_splitter.split_documents([doc])\n",
    "        semantic_chunks.extend(chunks)\n",
    "\n",
    "    # Store chunks and embeddings in Chroma for retrieval\n",
    "    vectordb = Chroma.from_documents(semantic_chunks, embeddings_model)\n",
    "\n",
    "    # Create a LangChain RetrievalQA pipeline\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        retriever=vectordb.as_retriever(),\n",
    "        llm=llm\n",
    "    )\n",
    "\n",
    "    # Embed the job description for similarity scoring\n",
    "    embeddings_model2 = OpenAIEmbeddings(model='text-embedding-3-small')  # Use the same model\n",
    "    job_desc_embedding = embeddings_model2.embed_query(job_description)\n",
    "\n",
    "    # Generate summaries and calculate similarity scores\n",
    "    results = []\n",
    "    education_level_mapping = {\n",
    "        'O level or equivalent': 1,\n",
    "        'A level or equivalent': 2,\n",
    "        'Diploma': 3,\n",
    "        'Degree': 4,\n",
    "        'Graduate Degree': 5,\n",
    "        'Other': 0\n",
    "    }\n",
    "\n",
    "    for idx, resume_text in enumerate(resume_texts):\n",
    "        # Get the embedding for the current resume\n",
    "        resume_embedding = embeddings_model.embed_query(resume_text)\n",
    "\n",
    "        # Calculate cosine similarity between job description and resume\n",
    "        similarity_score = cosine_similarity(\n",
    "            [job_desc_embedding], [resume_embedding]\n",
    "        )[0][0]\n",
    "\n",
    "        # Modify the prompt to request JSON-only output\n",
    "        result = qa_chain.run(f\"\"\"\n",
    "        For the following resume, extract and return the following details in JSON format only. Do not include any additional text or explanationsâ€”just the JSON data.\n",
    "\n",
    "        Required details:\n",
    "        - Name\n",
    "        - Education (list all tertiary degrees and above obtained, including degree name and field of study. If no tertiary degree then state the highest education achieved)\n",
    "        - Map the highest obtained education level to one of the following categories: \"O level or equivalent\", \"A level or equivalent\", \"Diploma\", \"Degree\", \"Graduate Degree\", \"Other\"\n",
    "        - Provide the mapped education level as \"Mapped Education Level\"\n",
    "        - Highlights of Career (2-3 standout sentences from the resume)\n",
    "\n",
    "        - Work experiences (list of job experiences), where each job experience includes:\n",
    "            - Job title\n",
    "            - Company name\n",
    "            - Start date\n",
    "            - End date\n",
    "\n",
    "        Example output:\n",
    "        {{\n",
    "            \"Name\": \"John Doe\",\n",
    "            \"Education\": [\n",
    "                \"Bachelor in Computer Science\",\n",
    "                \"Master of Business Administration\"\n",
    "            ],\n",
    "            \"Mapped Education Level\": \"Graduate Degree\",\n",
    "            \"Highlights of Career\": [\n",
    "                \"Led a team of developers to create a cutting-edge AI application.\",\n",
    "                \"Increased company revenue by 20% through innovative solutions.\"\n",
    "            ],\n",
    "            \"Work experiences\": [\n",
    "                {{\"Job title\": \"Software Engineer\", \"Company name\": \"ABC Corp\", \"Start date\": \"Jan 2020\", \"End date\": \"Present\"}},\n",
    "                {{\"Job title\": \"Junior Developer\", \"Company name\": \"XYZ Ltd\", \"Start date\": \"May 2017\", \"End date\": \"Dec 2019\"}}\n",
    "            ]\n",
    "        }}\n",
    "        Return only the JSON data. Do not include any additional text.\n",
    "\n",
    "        Resume: {resume_text[:1000]}...\n",
    "        \"\"\")\n",
    "\n",
    "        # Try parsing the result as JSON\n",
    "        try:\n",
    "            # Extract the JSON object from the result\n",
    "            json_match = re.search(r'\\{.*\\}', result, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group()\n",
    "                parsed_result = json.loads(json_str)\n",
    "            else:\n",
    "                raise ValueError(\"No JSON object found in the model's output.\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"Failed to parse result for {filenames[idx]}: {e}\")\n",
    "            st.write(\"Model's raw output:\")\n",
    "            st.write(result)\n",
    "            parsed_result = {}\n",
    "\n",
    "        name = parsed_result.get('Name', 'N/A')\n",
    "        education = parsed_result.get('Education', [])\n",
    "        mapped_education_level = parsed_result.get('Mapped Education Level', 'Other')\n",
    "        highlights = parsed_result.get('Highlights of Career', [])\n",
    "        job_experiences = parsed_result.get('Work experiences', [])\n",
    "\n",
    "        # Map candidate's education level to numeric value\n",
    "        candidate_education_level = education_level_mapping.get(mapped_education_level, 0)\n",
    "\n",
    "        # Proceed with processing job_experiences as before\n",
    "        formatted_roles = []\n",
    "        for job in job_experiences:\n",
    "            job_title = job.get('Job title', 'N/A')\n",
    "            company = job.get('Company name', 'N/A')\n",
    "            start_date = job.get('Start date', 'N/A')\n",
    "            end_date = job.get('End date', 'N/A')\n",
    "            role_text = f\"{job_title} in {company} ({start_date} - {end_date})\"\n",
    "\n",
    "            # Calculate relevance score for this specific job role\n",
    "            role_embedding = embeddings_model.embed_query(role_text)\n",
    "            relevance_score = cosine_similarity(\n",
    "                [job_desc_embedding], [role_embedding]\n",
    "            )[0][0] * 10  # Scale the relevance score (0-10 scale)\n",
    "\n",
    "            # Format the role with relevance score\n",
    "            formatted_roles.append(f\"{job_title} in {company} [Relevant score: {relevance_score:.1f}], {start_date} - {end_date}\")\n",
    "\n",
    "        # Append the summary, similarity score, and relevant roles to the results\n",
    "        results.append({\n",
    "            \"Filename\": filenames[idx],\n",
    "            \"Name\": name,\n",
    "            \"Education\": education,\n",
    "            \"Mapped Education Level\": mapped_education_level,\n",
    "            \"Candidate Education Level\": candidate_education_level,\n",
    "            \"Highlights\": highlights,\n",
    "            \"Relevant Years of Experience\": '\\n'.join(formatted_roles),\n",
    "            \"Similarity Score\": similarity_score,\n",
    "            \"Resume Text\": resume_text  # Include the resume text\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Add a button to run the resume matching\n",
    "if st.button(\"Match Resumes\"):\n",
    "    if resume_source == 'Manual Upload':\n",
    "        if job_description and uploaded_files:\n",
    "            st.session_state.matched_resumes = True\n",
    "            resume_texts = []\n",
    "            filenames = []\n",
    "            for uploaded_file in uploaded_files:\n",
    "                try:\n",
    "                    file_content = extract_text(uploaded_file)\n",
    "                    resume_texts.append(file_content)\n",
    "                    filenames.append(uploaded_file.name)\n",
    "                except Exception as e:\n",
    "                    st.error(f\"Error processing file {uploaded_file.name}: {e}\")\n",
    "            # Process resumes and store results\n",
    "            results = process_resumes(job_description, resume_texts, filenames)\n",
    "            st.session_state.results = results\n",
    "        else:\n",
    "            st.error(\"Please enter a job description and upload at least one resume.\")\n",
    "    elif resume_source == 'Automated Resume Extraction':\n",
    "        if job_title:\n",
    "            # Attempt to read job description from file\n",
    "            job_description_file = os.path.join('job_descriptions', f\"{job_title}.txt\")\n",
    "            if os.path.exists(job_description_file):\n",
    "                with open(job_description_file, 'r', encoding='utf-8') as f:\n",
    "                    job_description = f.read()\n",
    "                # Proceed to filter resumes based on submission date\n",
    "                filtered_resumes_info = [res for res in sample_resumes if start_date <= res['submission_date'] <= end_date]\n",
    "                if not filtered_resumes_info:\n",
    "                    st.warning(\"No resumes found for the selected date range.\")\n",
    "                else:\n",
    "                    st.session_state.matched_resumes = True\n",
    "                    filenames = [res['filename'] for res in filtered_resumes_info]\n",
    "                    resume_texts = []\n",
    "                    for res_info in filtered_resumes_info:\n",
    "                        filename = res_info['filename']\n",
    "                        file_path = os.path.join('resumes', filename)\n",
    "                        try:\n",
    "                            with open(file_path, 'rb') as f:\n",
    "                                file_content = extract_text(f)\n",
    "                                resume_texts.append(file_content)\n",
    "                        except Exception as e:\n",
    "                            st.error(f\"Error reading file {filename}: {e}\")\n",
    "                    # Process resumes and store results\n",
    "                    results = process_resumes(job_description, resume_texts, filenames)\n",
    "                    st.session_state.results = results\n",
    "            else:\n",
    "                st.error(f\"Job description file for '{job_title}' not found.\")\n",
    "        else:\n",
    "            st.error(\"Please enter a job title/code.\")\n",
    "\n",
    "# Only proceed if resumes have been matched and results are available\n",
    "if st.session_state.matched_resumes and st.session_state.results is not None:\n",
    "    results = st.session_state.results\n",
    "\n",
    "    # Map education levels to numeric values\n",
    "    education_level_mapping = {\n",
    "        'O level or equivalent': 1,\n",
    "        'A level or equivalent': 2,\n",
    "        'Diploma': 3,\n",
    "        'Degree': 4,\n",
    "        'Graduate Degree': 5,\n",
    "        'Other': 0\n",
    "    }\n",
    "\n",
    "    # Map user's selected minimum education level\n",
    "    selected_min_education_level = education_level_mapping.get(min_education, 0)\n",
    "\n",
    "    # Filter resumes based on minimum education level\n",
    "    filtered_results = [result for result in results if result['Candidate Education Level'] >= selected_min_education_level]\n",
    "\n",
    "    # Check if any resumes meet the criteria\n",
    "    if not filtered_results:\n",
    "        st.warning(\"No resumes meet the minimum education level requirement.\")\n",
    "    else:\n",
    "        # Sort filtered results by similarity score in descending order\n",
    "        filtered_results = sorted(filtered_results, key=lambda x: x[\"Similarity Score\"], reverse=True)\n",
    "\n",
    "        # Convert filtered results to DataFrame\n",
    "        df_results = pd.DataFrame(filtered_results)\n",
    "\n",
    "        # Add a column to number each row starting from 1\n",
    "        df_results.insert(0, ' ', pd.Series(range(1, len(df_results) + 1)))\n",
    "\n",
    "        # For columns that are lists (Education, Highlights), join them into strings\n",
    "        df_results['Education'] = df_results['Education'].apply(lambda x: '; '.join(x) if isinstance(x, list) else x)\n",
    "        df_results['Highlights'] = df_results['Highlights'].apply(lambda x: '; '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "        # Reorder columns if necessary\n",
    "        df_results = df_results[[' ', 'Filename', 'Name', 'Education', 'Mapped Education Level', 'Highlights', 'Relevant Years of Experience', 'Similarity Score', 'Resume Text']]\n",
    "\n",
    "        # Create a brief ranking above the table\n",
    "        st.subheader(\"Resume Rankings\")\n",
    "        for idx, row in df_results.iterrows():\n",
    "            if idx >= top_n:\n",
    "                break\n",
    "\n",
    "            # Use the ' ' column for the counter/numbering\n",
    "            counter = row[' ']  # Refers to the numbering column\n",
    "\n",
    "            # Create two columns: one for numbering and one for the expander\n",
    "            col1, col2 = st.columns([1, 9])  # Adjust the ratio as needed for layout\n",
    "\n",
    "            # Display the counter in the first column\n",
    "            with col1:\n",
    "                st.write(f\"{counter}\")\n",
    "\n",
    "            # Display the expander in the second column\n",
    "            with col2:\n",
    "                with st.expander(f\"{row['Filename']} - Similarity Score: {row['Similarity Score']:.2f}\"):\n",
    "                    st.write(row['Resume Text'])\n",
    "\n",
    "        # Display the DataFrame without modifying the 'Filename' column\n",
    "        st.write(\"Detailed Resume Summaries\")\n",
    "        # Exclude 'Resume Text' from the displayed DataFrame\n",
    "        df_display = df_results.drop(columns=['Resume Text'])\n",
    "        st.write(df_display.to_html(escape=False, index=False), unsafe_allow_html=True)\n",
    "\n",
    "        # Input field for hiring manager's email\n",
    "        recipient_email = st.text_input(\"Enter hiring manager's email:\")\n",
    "\n",
    "        # Button to send the summary table\n",
    "        if st.button(\"Forward Summary to Hiring Manager\"):\n",
    "            if recipient_email:\n",
    "                # Prepare DataFrame for CSV\n",
    "                # Replace '<br>' with newline characters '\\n' if necessary\n",
    "                df_results['Relevant Years of Experience'] = df_results['Relevant Years of Experience'].str.replace('<br>', '\\n')\n",
    "\n",
    "                # Exclude 'Resume Text' from the CSV\n",
    "                df_csv = df_results.drop(columns=['Resume Text'])\n",
    "                csv_data = df_csv.to_csv(index=False)\n",
    "\n",
    "                # Prepare the email content\n",
    "                subject = \"Resume Summaries\"\n",
    "                message = \"Dear Hiring Manager,\\n\\nPlease find attached the summaries of the top resumes.\\n\\nBest regards,\\nYour Name\"\n",
    "\n",
    "                # Send the email with attachment\n",
    "                success = send_email(\n",
    "                    sender_email, sender_password, recipient_email,\n",
    "                    subject, message,\n",
    "                    attachment=csv_data.encode('utf-8-sig'),\n",
    "                    attachment_filename='resume_summaries.csv'\n",
    "                )\n",
    "                if success:\n",
    "                    st.success(f\"Email sent to {recipient_email}\")\n",
    "                else:\n",
    "                    st.error(f\"Failed to send email to {recipient_email}\")\n",
    "            else:\n",
    "                st.error(\"Please enter a recipient email address.\")\n",
    "else:\n",
    "    if st.session_state.matched_resumes:\n",
    "        st.info(\"Processing resumes. Please wait...\")\n",
    "    else:\n",
    "        st.info(\"Please click 'Match Resumes' to process the resumes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8bc4490a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "##final!!!!!\n",
    "!streamlit run resume_screen_testapp2.py --server.enableXsrfProtection false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "92afdbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1_About_Us.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 1_About_Us.py\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"About Us\")\n",
    "\n",
    "st.header(\"Project Scope and Objectives\")\n",
    "st.write(\"\"\"\n",
    "The **Resume Matcher with Advanced Retrieval** is designed to streamline the recruitment process by leveraging artificial intelligence and natural language processing technologies. Our application aims to:\n",
    "\n",
    "- **Automate resume extraction and matching** to job descriptions.\n",
    "- **Rank candidates** based on relevance and predefined criteria.\n",
    "- **Enhance efficiency** in the recruitment workflow.\n",
    "- **Provide insightful summaries** to assist hiring decisions.\n",
    "\"\"\")\n",
    "\n",
    "st.header(\"Data Sources\")\n",
    "st.write(\"\"\"\n",
    "- **Resumes**: Uploaded manually by users or extracted automatically from a simulated database based on submission dates.\n",
    "- **Job Descriptions**: Retrieved automatically using job title or code, simulating an integration with an Applicant Tracking System (ATS) or database.\n",
    "\"\"\")\n",
    "\n",
    "st.header(\"Features\")\n",
    "st.write(\"\"\"\n",
    "- **Automated Resume Extraction**: Fetch resumes submitted within a specified date range.\n",
    "- **Manual Resume Upload**: Upload resumes directly for processing.\n",
    "- **Job Description Retrieval**: Obtain job descriptions based on job title or code.\n",
    "- **Advanced Matching and Ranking**: Use AI models to match and rank resumes.\n",
    "- **Education Level Filtering**: Filter candidates by minimum education requirements.\n",
    "- **Email Forwarding**: Send summarized results to hiring managers.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "257a1b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 2_Methodology.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 2_Methodology.py\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"Methodology\")\n",
    "\n",
    "st.header(\"Data Flow and Implementation Details\")\n",
    "\n",
    "st.subheader(\"Overview\")\n",
    "st.write(\"\"\"\n",
    "Our application processes resumes and job descriptions through several key steps to match and rank candidates effectively.\n",
    "\"\"\")\n",
    "\n",
    "st.subheader(\"1. Data Input\")\n",
    "st.write(\"\"\"\n",
    "- **Automated Resume Extraction**:\n",
    "  - Users input a job title/code.\n",
    "  - Job descriptions are retrieved from pre-defined files.\n",
    "  - Resumes are filtered by submission date.\n",
    "- **Manual Upload**:\n",
    "  - Users input a job description manually.\n",
    "  - Resumes are uploaded directly by the user.\n",
    "\"\"\")\n",
    "\n",
    "st.subheader(\"2. Text Extraction\")\n",
    "st.write(\"\"\"\n",
    "- Resumes and job descriptions are processed to extract text content.\n",
    "- Supports multiple file formats: PDF, DOCX, TXT.\n",
    "\"\"\")\n",
    "\n",
    "st.subheader(\"3. Data Processing\")\n",
    "st.write(\"\"\"\n",
    "- **Semantic Chunking**:\n",
    "  - Resumes are split into meaningful chunks using `SemanticChunker` for better context understanding.\n",
    "- **Embedding Generation**:\n",
    "  - Text embeddings are generated using OpenAI's embedding models.\n",
    "- **Similarity Scoring**:\n",
    "  - Cosine similarity is calculated between job description and resume embeddings to assess relevance.\n",
    "- **Information Extraction**:\n",
    "  - An LLM (e.g., GPT-4) extracts structured data from resumes, such as education, work experience, and highlights.\n",
    "\"\"\")\n",
    "\n",
    "st.subheader(\"4. Filtering and Ranking\")\n",
    "st.write(\"\"\"\n",
    "- Resumes are filtered based on the minimum education level specified.\n",
    "- Candidates are ranked according to similarity scores.\n",
    "\"\"\")\n",
    "\n",
    "st.subheader(\"5. Output and Visualization\")\n",
    "st.write(\"\"\"\n",
    "- Results are displayed in a ranked list with detailed summaries.\n",
    "- Users can adjust filters and view the top N candidates.\n",
    "- Provides an option to forward the summaries via email.\n",
    "\"\"\")\n",
    "\n",
    "st.subheader(\"Flowcharts\")\n",
    "st.write(\"The following flowcharts illustrate the process flow for each use case:\")\n",
    "\n",
    "# Include flowchart images (replace with your actual image paths)\n",
    "#st.image('images/manual_upload_flowchart.png', caption='Manual Upload Process Flow', use_column_width=True)\n",
    "#st.image('images/automated_extraction_flowchart.png', caption='Automated Resume Extraction Process Flow', use_column_width=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "63bc98b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Home.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Home.py\n",
    "\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"Resume Matcher\",\n",
    "    page_icon=\":page_with_curl:\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\",\n",
    ")\n",
    "\n",
    "\n",
    "def add_bg_from_url():\n",
    "    st.markdown(\n",
    "         f\"\"\"\n",
    "         <style>\n",
    "         .stApp {{\n",
    "             #background-image: url(\"https://cdn.pixabay.com/photo/2016/10/29/10/12/purple-1780371_1280.png\");\n",
    "             background-image: linear-gradient(rgba(0, 0, 0, 0.5), rgba(255, 255, 255, 0.75)), url(\"https://cdn.pixabay.com/photo/2016/10/29/10/12/purple-1780371_1280.png\");\n",
    "             background-attachment: fixed;\n",
    "             background-size: cover\n",
    "         }}\n",
    "         /* Ensure the font color stays black */\n",
    "         .stApp * {{\n",
    "             color: black !important;\n",
    "         }}\n",
    "         </style>\n",
    "         \"\"\",\n",
    "         unsafe_allow_html=True\n",
    "     )\n",
    "\n",
    "add_bg_from_url() \n",
    "\n",
    "\n",
    "# Path to your header image\n",
    "header_image_path = \"C:/Users/weichee/Documents/ai bootcamp/headerc.png\"\n",
    "header_image = Image.open(header_image_path)\n",
    "header_image_path2 = \"C:/Users/weichee/Documents/ai bootcamp/screenshot2.png\"\n",
    "header_image2 = Image.open(header_image_path2)\n",
    "\n",
    "# Display the header image\n",
    "col1 = st.columns(1)[0]\n",
    "with col1:\n",
    "    st.image(header_image, caption=\"\", use_column_width=True)\n",
    "    \n",
    "col1 = st.columns(1)[0]\n",
    "with col1:\n",
    "    st.image(header_image2, caption=\"\", use_column_width=True)\n",
    "\n",
    "\n",
    "st.subheader(\"\"\"\n",
    "Welcome to the Resume Matcher application. Use the navigation menu on the left to access different sections of the app.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8bfd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!streamlit run Home.py --server.enableXsrfProtection false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513d4f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing testing2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile testing2.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "import PyPDF2\n",
    "import json\n",
    "import re\n",
    "import openai\n",
    "import os\n",
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.docstore.document import Document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.application import MIMEApplication\n",
    "import tiktoken\n",
    "# from helper_functions.utility import check_password  # Commented out since we're not using helper functions\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "if load_dotenv('.env'):\n",
    "    # For local development\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    sender_email = os.getenv('SENDER_EMAIL')\n",
    "    sender_password = os.getenv('SENDER_PASSWORD')\n",
    "else:\n",
    "    openai_api_key = st.secrets['OPENAI_API_KEY']\n",
    "    sender_email = st.secrets['SENDER_EMAIL']\n",
    "    sender_password = st.secrets['SENDER_PASSWORD']\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Function to extract text from PDF files\n",
    "def extract_text_from_pdf(file):\n",
    "    text = \"\"\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to extract text from DOCX files\n",
    "def extract_text_from_docx(file):\n",
    "    return docx2txt.process(file)\n",
    "\n",
    "# Function to extract text from TXT files\n",
    "def extract_text_from_txt(file):\n",
    "    return file.read().decode(\"utf-8\")\n",
    "\n",
    "# Function to extract text from different file types\n",
    "def extract_text(uploaded_file):\n",
    "    if uploaded_file.name.lower().endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(uploaded_file)\n",
    "    elif uploaded_file.name.lower().endswith(\".docx\"):\n",
    "        return extract_text_from_docx(uploaded_file)\n",
    "    elif uploaded_file.name.lower().endswith(\".txt\"):\n",
    "        return extract_text_from_txt(uploaded_file)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Function to send email with attachment\n",
    "def send_email(sender_email, sender_password, recipient_email, subject, message, attachment=None, attachment_filename='attachment.csv'):\n",
    "    try:\n",
    "        # Create the email message\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = sender_email\n",
    "        msg['To'] = recipient_email\n",
    "        msg['Subject'] = subject\n",
    "\n",
    "        # Add message body\n",
    "        msg.attach(MIMEText(message, 'plain'))\n",
    "\n",
    "        # Attach the CSV file if provided\n",
    "        if attachment is not None:\n",
    "            part = MIMEApplication(attachment, Name=attachment_filename)\n",
    "            part['Content-Disposition'] = f'attachment; filename=\"{attachment_filename}\"'\n",
    "            msg.attach(part)\n",
    "\n",
    "        # Connect to SMTP server using STARTTLS\n",
    "        server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "        server.starttls()\n",
    "        server.login(sender_email, sender_password)\n",
    "\n",
    "        # Send the email\n",
    "        server.sendmail(sender_email, recipient_email, msg.as_string())\n",
    "        server.quit()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error sending email: {e}\")\n",
    "        return False\n",
    "\n",
    "# Set the page layout to wide\n",
    "st.set_page_config(layout=\"wide\")\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"Resume Matcher with Advanced Retrieval\")\n",
    "\n",
    "# Check if the password is correct.\n",
    "# For this example, we'll skip password checking\n",
    "# if not check_password():\n",
    "#     st.stop()\n",
    "\n",
    "# Initialize session state variables\n",
    "if 'matched_resumes' not in st.session_state:\n",
    "    st.session_state.matched_resumes = False\n",
    "if 'results' not in st.session_state:\n",
    "    st.session_state.results = None\n",
    "\n",
    "# Sample resumes with submission dates (simulate automated extraction)\n",
    "sample_resumes = [\n",
    "    {'filename': 'resume1_KENNY SIM.docx', 'submission_date': datetime.date(2023, 9, 1)},\n",
    "    {'filename': 'resume2_crystal.docx', 'submission_date': datetime.date(2023, 9, 5)},\n",
    "    {'filename': 'resume3_mark.docx', 'submission_date': datetime.date(2023, 9, 10)},\n",
    "    {'filename': 'resume4_david.docx', 'submission_date': datetime.date(2023, 9, 15)},\n",
    "    {'filename': 'resume5_linus_ds.docx', 'submission_date': datetime.date(2023, 10, 15)}\n",
    "]\n",
    "\n",
    "# Create tabs for resume source selection\n",
    "tabs = st.tabs([\"Automated Resume Extraction\", \"Manual Upload\"])\n",
    "\n",
    "# -------------------- Common Inputs Section --------------------\n",
    "st.markdown(\"### Common Inputs\")\n",
    "\n",
    "with st.form(\"common_inputs\"):\n",
    "    # Input for number of top resumes to display\n",
    "    top_n = st.number_input(\"Enter the number of top resumes to display\", min_value=1, value=5, step=1)\n",
    "\n",
    "    # Dropdown for minimum education level\n",
    "    education_levels = ['None', 'O level or equivalent', 'A level or equivalent', 'Diploma', 'Degree', 'Graduate Degree']\n",
    "    min_education = st.selectbox(\"Select minimum education level required:\", education_levels, index=0)\n",
    "\n",
    "    # Submit button for common inputs\n",
    "    show_results = st.form_submit_button(\"Show Results\")\n",
    "\n",
    "# -------------------- Automated Resume Extraction Tab --------------------\n",
    "with tabs[0]:\n",
    "    st.markdown(\"### Automated Resume Extraction Inputs\")\n",
    "    with st.form(\"automated_resume_extraction\"):\n",
    "        st.markdown(\"### Automated Resume Extraction Inputs\")\n",
    "        \n",
    "        # Input for job title/job code\n",
    "        job_title = st.text_input(\"Enter Job Title/Code\", value='e.g. Data Scientist JD01')\n",
    "\n",
    "        # Date range picker for resume submission date\n",
    "        date_range = st.date_input(\"Select Resume Submission Date Range\", value=(datetime.date(2023, 9, 1), datetime.date(2023, 9, 30)))\n",
    "        if isinstance(date_range, tuple):\n",
    "            start_date, end_date = date_range\n",
    "        else:\n",
    "            start_date = end_date = date_range\n",
    "\n",
    "        # Submit button\n",
    "        submitted_automated = st.form_submit_button(\"Extract Resume\")\n",
    "\n",
    "# -------------------- Manual Upload Tab --------------------\n",
    "with tabs[1]:\n",
    "    st.markdown(\"### Manual Upload Inputs\")\n",
    "    with st.form(\"manual_upload\"):\n",
    "        st.markdown(\"### Manual Upload Inputs\")\n",
    "        \n",
    "        # Input for job description\n",
    "        job_description = st.text_area(\"Enter Job Description\")\n",
    "\n",
    "        # Upload resumes\n",
    "        uploaded_files = st.file_uploader(\"Upload Resumes\", accept_multiple_files=True, type=[\"pdf\", \"docx\", \"txt\"])\n",
    "\n",
    "        # Submit button\n",
    "        submitted_manual = st.form_submit_button(\"Extract Resume\")\n",
    "\n",
    "# Function to process resumes (unchanged)\n",
    "def process_resumes(job_description, resume_texts, filenames):\n",
    "    # Use the SemanticChunker to split resumes into meaningful chunks\n",
    "    embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    text_splitter = SemanticChunker(embeddings_model)  # Semantic chunker instead of token-based\n",
    "\n",
    "    # Wrap text content into Document objects for LangChain processing\n",
    "    documents = [Document(page_content=text, metadata={\"filename\": filenames[i]}) for i, text in enumerate(resume_texts)]\n",
    "    semantic_chunks = []\n",
    "    for doc in documents:\n",
    "        chunks = text_splitter.split_documents([doc])\n",
    "        semantic_chunks.extend(chunks)\n",
    "\n",
    "    # Store chunks and embeddings in Chroma for retrieval\n",
    "    vectordb = Chroma.from_documents(semantic_chunks, embeddings_model)\n",
    "\n",
    "    # Create a LangChain RetrievalQA pipeline\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        retriever=vectordb.as_retriever(),\n",
    "        llm=llm\n",
    "    )\n",
    "\n",
    "    # Embed the job description for similarity scoring\n",
    "    embeddings_model2 = OpenAIEmbeddings(model='text-embedding-3-small')  # Use the same model\n",
    "    job_desc_embedding = embeddings_model2.embed_query(job_description)\n",
    "\n",
    "    # Generate summaries and calculate similarity scores\n",
    "    results = []\n",
    "    education_level_mapping = {\n",
    "        'O level or equivalent': 1,\n",
    "        'A level or equivalent': 2,\n",
    "        'Diploma': 3,\n",
    "        'Degree': 4,\n",
    "        'Graduate Degree': 5,\n",
    "        'Other': 0\n",
    "    }\n",
    "\n",
    "    for idx, resume_text in enumerate(resume_texts):\n",
    "        # Get the embedding for the current resume\n",
    "        resume_embedding = embeddings_model.embed_query(resume_text)\n",
    "\n",
    "        # Calculate cosine similarity between job description and resume\n",
    "        similarity_score = cosine_similarity(\n",
    "            [job_desc_embedding], [resume_embedding]\n",
    "        )[0][0]\n",
    "\n",
    "        # Modify the prompt to request JSON-only output\n",
    "        result = qa_chain.run(f\"\"\"\n",
    "        For the following resume, extract and return the following details in JSON format only. Do not include any additional text or explanationsâ€”just the JSON data.\n",
    "\n",
    "        Required details:\n",
    "        - Name\n",
    "        - Education (list all tertiary degrees and above obtained, including degree name and field of study. If no tertiary degree then state the highest education achieved)\n",
    "        - Map the highest obtained education level to one of the following categories: \"O level or equivalent\", \"A level or equivalent\", \"Diploma\", \"Degree\", \"Graduate Degree\", \"Other\"\n",
    "        - Provide the mapped education level as \"Mapped Education Level\"\n",
    "        - Highlights of Career (2-3 standout sentences from the resume)\n",
    "\n",
    "        - Work experiences (list of job experiences), where each job experience includes:\n",
    "            - Job title\n",
    "            - Company name\n",
    "            - Start date\n",
    "            - End date\n",
    "\n",
    "        Example output:\n",
    "        {{\n",
    "            \"Name\": \"John Doe\",\n",
    "            \"Education\": [\n",
    "                \"Bachelor in Computer Science\",\n",
    "                \"Master of Business Administration\"\n",
    "            ],\n",
    "            \"Mapped Education Level\": \"Graduate Degree\",\n",
    "            \"Highlights of Career\": [\n",
    "                \"Led a team of developers to create a cutting-edge AI application.\",\n",
    "                \"Increased company revenue by 20% through innovative solutions.\"\n",
    "            ],\n",
    "            \"Work experiences\": [\n",
    "                {{\"Job title\": \"Software Engineer\", \"Company name\": \"ABC Corp\", \"Start date\": \"Jan 2020\", \"End date\": \"Present\"}},\n",
    "                {{\"Job title\": \"Junior Developer\", \"Company name\": \"XYZ Ltd\", \"Start date\": \"May 2017\", \"End date\": \"Dec 2019\"}}\n",
    "            ]\n",
    "        }}\n",
    "        Return only the JSON data. Do not include any additional text.\n",
    "\n",
    "        Resume: {resume_text}...\n",
    "        \"\"\")\n",
    "\n",
    "        # Try parsing the result as JSON\n",
    "        try:\n",
    "            # Extract the JSON object from the result\n",
    "            json_match = re.search(r'\\{.*\\}', result, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group()\n",
    "                parsed_result = json.loads(json_str)\n",
    "            else:\n",
    "                raise ValueError(\"No JSON object found in the model's output.\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"Failed to parse result for {filenames[idx]}: {e}\")\n",
    "            st.write(\"Model's raw output:\")\n",
    "            st.write(result)\n",
    "            parsed_result = {}\n",
    "\n",
    "        name = parsed_result.get('Name', 'N/A')\n",
    "        education = parsed_result.get('Education', [])\n",
    "        mapped_education_level = parsed_result.get('Mapped Education Level', 'Other')\n",
    "        highlights = parsed_result.get('Highlights of Career', [])\n",
    "        job_experiences = parsed_result.get('Work experiences', [])\n",
    "\n",
    "        # Map candidate's education level to numeric value\n",
    "        candidate_education_level = education_level_mapping.get(mapped_education_level, 0)\n",
    "\n",
    "        # Proceed with processing job_experiences as before\n",
    "        formatted_roles = []\n",
    "        for job in job_experiences:\n",
    "            job_title_role = job.get('Job title', 'N/A')\n",
    "            company = job.get('Company name', 'N/A')\n",
    "            start_date_role = job.get('Start date', 'N/A')\n",
    "            end_date_role = job.get('End date', 'N/A')\n",
    "            role_text = f\"{job_title_role} in {company} ({start_date_role} - {end_date_role})\"\n",
    "\n",
    "            # Calculate relevance score for this specific job role\n",
    "            role_embedding = embeddings_model.embed_query(role_text)\n",
    "            relevance_score = cosine_similarity(\n",
    "                [job_desc_embedding], [role_embedding]\n",
    "            )[0][0] * 10  # Scale the relevance score (0-10 scale)\n",
    "\n",
    "            # Format the role with relevance score\n",
    "            formatted_roles.append(f\"{job_title_role} in {company} [Relevant score: {relevance_score:.1f}], {start_date_role} - {end_date_role}\")\n",
    "\n",
    "        # Append the summary, similarity score, and relevant roles to the results\n",
    "        results.append({\n",
    "            \"Filename\": filenames[idx],\n",
    "            \"Name\": name,\n",
    "            \"Education\": education,\n",
    "            \"Mapped Education Level\": mapped_education_level,\n",
    "            \"Candidate Education Level\": candidate_education_level,\n",
    "            \"Highlights\": highlights,\n",
    "            \"Relevant Years of Experience\": '\\n'.join(formatted_roles),\n",
    "            \"Similarity Score\": similarity_score,\n",
    "            \"Resume Text\": resume_text  # Include the resume text\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# -------------------- Handle \"Extract Resume\" Submission --------------------\n",
    "# Handle \"Extract Resume\" submission from Automated Resume Extraction tab\n",
    "if submitted_automated:\n",
    "    # Automated Resume Extraction logic\n",
    "    if job_title:\n",
    "        # Attempt to read job description from file\n",
    "        job_description_file = os.path.join('job_descriptions', f\"{job_title}.txt\")\n",
    "        if os.path.exists(job_description_file):\n",
    "            with open(job_description_file, 'r', encoding='utf-8') as f:\n",
    "                job_description = f.read()\n",
    "            # Proceed to filter resumes based on submission date\n",
    "            filtered_resumes_info = [res for res in sample_resumes if start_date <= res['submission_date'] <= end_date]\n",
    "            if not filtered_resumes_info:\n",
    "                st.warning(\"No resumes found for the selected date range.\")\n",
    "            else:\n",
    "                # Extract and process resumes\n",
    "                filenames = [res['filename'] for res in filtered_resumes_info]\n",
    "                resume_texts = []\n",
    "                for res_info in filtered_resumes_info:\n",
    "                    filename = res_info['filename']\n",
    "                    file_path = os.path.join('resumes', filename)\n",
    "                    try:\n",
    "                        with open(file_path, 'rb') as f:\n",
    "                            file_content = extract_text(f)\n",
    "                            resume_texts.append(file_content)\n",
    "                    except Exception as e:\n",
    "                        st.error(f\"Error reading file {filename}: {e}\")\n",
    "                # Process resumes and store results\n",
    "                with st.spinner('Processing resumes...'):\n",
    "                    results = process_resumes(job_description, resume_texts, filenames)\n",
    "                st.success(\"Resumes extracted and processed successfully!\")\n",
    "                st.session_state.matched_resumes = True\n",
    "                st.session_state.results = results\n",
    "        else:\n",
    "            st.error(f\"Job description file for '{job_title}' not found.\")\n",
    "    else:\n",
    "        st.error(\"Please enter a job title/code.\")\n",
    "\n",
    "# Handle \"Extract Resume\" submission from Manual Upload tab\n",
    "if submitted_manual:\n",
    "    # Manual Upload logic\n",
    "    if job_description and uploaded_files:\n",
    "        # Extract text from uploaded files\n",
    "        resume_texts = []\n",
    "        filenames = []\n",
    "        for uploaded_file in uploaded_files:\n",
    "            try:\n",
    "                file_content = extract_text(uploaded_file)\n",
    "                resume_texts.append(file_content)\n",
    "                filenames.append(uploaded_file.name)\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error processing file {uploaded_file.name}: {e}\")\n",
    "        # Process resumes and store results\n",
    "        with st.spinner('Processing resumes...'):\n",
    "            results = process_resumes(job_description, resume_texts, filenames)\n",
    "        st.success(\"Resumes uploaded and processed successfully!\")\n",
    "        st.session_state.matched_resumes = True\n",
    "        st.session_state.results = results\n",
    "    else:\n",
    "        st.error(\"Please enter a job description and upload at least one resume.\")\n",
    "\n",
    "# -------------------- Handle \"Show Results\" Submission --------------------\n",
    "if show_results:\n",
    "    if st.session_state.matched_resumes and st.session_state.results is not None:\n",
    "        results = st.session_state.results\n",
    "\n",
    "        # Map education levels to numeric values\n",
    "        education_level_mapping = {\n",
    "            'O level or equivalent': 1,\n",
    "            'A level or equivalent': 2,\n",
    "            'Diploma': 3,\n",
    "            'Degree': 4,\n",
    "            'Graduate Degree': 5,\n",
    "            'Other': 0\n",
    "        }\n",
    "\n",
    "        # Map user's selected minimum education level\n",
    "        selected_min_education_level = education_level_mapping.get(min_education, 0)\n",
    "\n",
    "        # Filter resumes based on minimum education level\n",
    "        filtered_results = [result for result in results if result['Candidate Education Level'] >= selected_min_education_level]\n",
    "\n",
    "        # Check if any resumes meet the criteria\n",
    "        if not filtered_results:\n",
    "            st.warning(\"No resumes meet the minimum education level requirement.\")\n",
    "        else:\n",
    "            # Sort filtered results by similarity score in descending order\n",
    "            filtered_results = sorted(filtered_results, key=lambda x: x[\"Similarity Score\"], reverse=True)\n",
    "\n",
    "            # Convert filtered results to DataFrame\n",
    "            df_results = pd.DataFrame(filtered_results)\n",
    "\n",
    "            # Add a column to number each row starting from 1\n",
    "            df_results.insert(0, 'No.', pd.Series(range(1, len(df_results) + 1)))\n",
    "\n",
    "            # For columns that are lists (Education, Highlights), join them into strings\n",
    "            df_results['Education'] = df_results['Education'].apply(lambda x: '; '.join(x) if isinstance(x, list) else x)\n",
    "            df_results['Highlights'] = df_results['Highlights'].apply(lambda x: '; '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "            # Reorder columns if necessary\n",
    "            df_results = df_results[['No.', 'Filename', 'Name', 'Education', 'Mapped Education Level', 'Highlights', 'Relevant Years of Experience', 'Similarity Score', 'Resume Text']]\n",
    "\n",
    "            # Apply the top_n filter to both Resume Rankings and Detailed Resume Summaries\n",
    "            top_n_filtered = min(top_n, len(df_results))  # Ensure top_n does not exceed available resumes\n",
    "\n",
    "            # Store results in session state\n",
    "            st.session_state['df_results'] = df_results\n",
    "            st.session_state['top_n_filtered'] = top_n_filtered\n",
    "    else:\n",
    "        if st.session_state.matched_resumes:\n",
    "            st.info(\"Processing resumes. Please wait...\")\n",
    "        else:\n",
    "            st.info(\"Please fill out the form and submit to process the resumes.\")\n",
    "\n",
    "# -------------------- Display Results and Handle Email Sending --------------------\n",
    "# Check if df_results is available in session state\n",
    "if 'df_results' in st.session_state:\n",
    "    df_results = st.session_state['df_results']\n",
    "    top_n_filtered = st.session_state['top_n_filtered']\n",
    "\n",
    "    # Create a brief ranking above the table\n",
    "    st.subheader(\"Resume Rankings\")\n",
    "    for idx, row in df_results.iterrows():\n",
    "        if idx >= top_n_filtered:\n",
    "            break\n",
    "\n",
    "        # Use the 'No.' column for the counter/numbering\n",
    "        counter = row['No.']  # Refers to the numbering column\n",
    "\n",
    "        # Create two columns: one for numbering and one for the expander\n",
    "        col1, col2 = st.columns([0.5, 9.5])  # Adjust the ratio as needed for layout\n",
    "\n",
    "        # Display the counter in the first column\n",
    "        with col1:\n",
    "            st.write(f\"{counter}\")\n",
    "\n",
    "        # Display the expander in the second column\n",
    "        with col2:\n",
    "            with st.expander(f\"{row['Filename']} - Similarity Score: {row['Similarity Score']:.2f}\"):\n",
    "                st.write(row['Resume Text'])\n",
    "\n",
    "    # Display the DataFrame without modifying the 'Filename' column\n",
    "    st.subheader(\"Detailed Resume Summaries\")\n",
    "    # Exclude 'Resume Text' from the displayed DataFrame and apply top_n filter\n",
    "    df_display = df_results.drop(columns=['Resume Text']).head(top_n_filtered)\n",
    "\n",
    "    st.write(df_display.to_html(escape=False, index=False), unsafe_allow_html=True)\n",
    "\n",
    "    # Input field for hiring manager's email\n",
    "    recipient_email = st.text_input(\"Enter hiring manager's email:\")\n",
    "\n",
    "    # Button to send the summary table\n",
    "    if st.button(\"Forward Summary to Hiring Manager\"):\n",
    "        if recipient_email:\n",
    "            # Prepare DataFrame for CSV\n",
    "            # Replace '<br>' with newline characters '\\n' if necessary\n",
    "            df_results['Relevant Years of Experience'] = df_results['Relevant Years of Experience'].str.replace('<br>', '\\n')\n",
    "\n",
    "            df_csv = df_results.drop(columns=['Resume Text'])\n",
    "            csv_data = df_csv.to_csv(index=False)\n",
    "\n",
    "            # Prepare the email content\n",
    "            subject = \"Resume Summaries\"\n",
    "            message = \"Dear Hiring Manager,\\n\\nPlease find attached the summaries of the top resumes.\\n\\nBest regards,\\nYour Name\"\n",
    "\n",
    "            # Send the email with attachment\n",
    "            success = send_email(\n",
    "                sender_email, sender_password, recipient_email,\n",
    "                subject, message,\n",
    "                attachment=csv_data.encode('utf-8-sig'),\n",
    "                attachment_filename='resume_summaries.csv'\n",
    "            )\n",
    "            if success:\n",
    "                st.success(f\"Email sent to {recipient_email}\")\n",
    "            else:\n",
    "                st.error(f\"Failed to send email to {recipient_email}\")\n",
    "        else:\n",
    "            st.error(\"Please enter a recipient email address.\")\n",
    "else:\n",
    "    st.info(\"Please fill out the form and submit to process the resumes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea200294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run testing2.py --server.enableXsrfProtection false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c20ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
